{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías para el análisis de datos\n",
    "import numpy as np  # Biblioteca para operaciones numéricas, especialmente útil para trabajar con matrices y arrays\n",
    "import pandas as pd  # Biblioteca para manipulación y análisis de datos, especialmente para DataFrames\n",
    "import statsmodels.api as sm  # Biblioteca para realizar modelos estadísticos, útil para regresiones y otros análisis estadísticos\n",
    "from collections import Counter  # Contador de elementos en colecciones, útil para contar la frecuencia de elementos en una lista o array\n",
    "\n",
    "# Librería para optimización\n",
    "from gurobipy import Model, GRB, quicksum  # Gurobi es una biblioteca de optimización matemática. \n",
    "# Model permite definir el modelo de optimización,\n",
    "# GRB contiene constantes (como tipos de variables y sentido de optimización),\n",
    "# quicksum permite realizar sumas de forma rápida y eficiente en Gurobi.\n",
    "\n",
    "# Librerías operacionales\n",
    "import os  # Biblioteca para interactuar con el sistema operativo (ej. manejo de archivos y rutas)\n",
    "from datetime import datetime  # Módulo para trabajar con fechas y tiempos, útil para capturar fechas actuales o manipular datos de tiempo\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# rl\n",
    "# # Importar las librerías necesarias\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import gc\n",
    "import logging  \n",
    "from sklearn.preprocessing import normalize\n",
    "from joblib import Parallel, delayed \n",
    "import warnings\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2>1. Carga de datos desde excel a dataframes de pandas</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se cargan los datos iniciales desde los csv que nos proveen. (Esto puede tardar un ratito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_tratamiento = 'https://drive.usercontent.google.com/download?id=1KTRwYGaWoQQnZwbk8VpdxKULMaOReoF5&authuser=0&confirm=t&uuid=4e4d7983-c5a5-412d-9f5b-b9bda1068b73&at=AENtkXZSxkDr84kWrDlz6ANq4ov2%3A1730951312566'\n",
    "\n",
    "df_tratamiento = pd.read_csv(url_tratamiento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '173bRZQG7NWfdpHJ-o4-NA3ieH1-Fhyok'\n",
    "url_informacion_de_clientes = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "df_informacion_de_clientes = pd.read_csv(url_informacion_de_clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_simulaciones_clientes = 'https://drive.usercontent.google.com/download?id=1IXyKwtKFLCUsAV1MtNqktiwKaHzV2D5A&authuser=0&confirm=t&uuid=edd22376-238f-4c1f-b603-728b07bafd7f&at=AENtkXaEURpV52p_BdWxyisvjhSQ%3A1730951026384'\n",
    "\n",
    "df_simulaciones_clientes = pd.read_csv(url_simulaciones_clientes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_id = '1Z4jMzZeD2q-4-ioSgyimppAdLZzqDkt3'\n",
    "url_ventas = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "df_ventas = pd.read_csv(url_ventas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3>1.1 Carga de 'Informacion_Clientes.csv'</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar se cargará la información de los clientes. Esto incluye las siguientes características de los clientes:\n",
    "\n",
    "* **unnamed**: algo como uid\n",
    "* **Rut**: identificador de Chile (supongo que por privacidad va desde 0 a max de observaciones)\n",
    "* **Género**: Masculino o femenino\n",
    "* **Categoría_Digital**: Si el cliente es digital o no\n",
    "* **Elasticidad_Precios**: Baja, media o alta\n",
    "* **Nacionalidad**: Chileno o extranjero\n",
    "* **Propensión**: Número entre 0 y 1 que idica que tan propenso a cursar un credito es el cliente\n",
    "* **Probabilidad_No_Pago**: Número entre 0 y 1 que indica la probabilidad de que el cliente no pague la deuda\n",
    "* **Edad**: Numero entero de edad en años\n",
    "* **Renta**: Renta promedio de los últimos 12 meses\n",
    "* **Oferta_Consumo**: Monto máximo que puede cursar un cliente dado sus antecedentes crediticios y situación socioeconómica. \n",
    "* **Deuda_CMF**: Deuda que tiene el cliente en otros bancos. Efectivamente es deuda pendiente, pero de créditos otorgados por la competencia.\n",
    "* **Tiempo_como_cliente**: Número de tiempo(no sé en que medida está) que el cliente lleva en el banco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se elimina el tiempo como cliente ya que no aporta información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_informacion_de_clientes.drop(columns=['Tiempo_como_cliente'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3>1.2 Carga de 'Simulaciones_Clientes.csv'</h3>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segundo lugar se cargaran las simulaciones hechas por los clientes en la página del banco. Esto incluye las siguientes características de las simulaciones:\n",
    "* **unnamed**: Supongo que es el número de simulacion registrada, un tipo de identificador de la simulación\n",
    "* **fecha**: yyyy-mm-dd fecha de la simulación\n",
    "* **rut**: identificador de Chile del cliente que hizo la simulacion\n",
    "* **monto_simulado**: monto prestado al cliente\n",
    "* **plazo_simulado**: plazo en **meses** del crédito\n",
    "* **tasa_simulado**: costo para el cliente del credito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulaciones_clientes = df_simulaciones_clientes[df_simulaciones_clientes['Monto_Simulado'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3>1.3 Carga de 'Tratamiento.csv'</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En tercer lugar se cargara el tratamiento que ha tenido el banco con el cliente, es decir, cómo se han contactado con él. Esto incluye las siguientes características:\n",
    "\n",
    "* **unnamed**: Número de tratamiento registrado\n",
    "* **fecha**: yyyy-mm-dd\n",
    "* **rut**: Identificador de Chile del cliente con el que se tiene el tipo de trato\n",
    "* **n_correos**: Cantidad de correos que se enviaron en el mes que sale la fecha. Es decir, si sele fecha '2024-03-01', correspondería a los correos enviados en marzo de 2024.\n",
    "* **asg_ejec**: Si el cliente tiene un ejecutivo asignado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <h3>1.4 Carga de 'Ventas.csv'</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último se cargaran las ventas que ha tenido el banco con el cliente. Esto incluye las siguientes características:\n",
    "\n",
    "* **unnamed**: Índice sin significado\n",
    "* **fecha**: yyyy-mm-dd -> fecha en la que se concretó la venta\n",
    "* **rut**: identificador de Chile del cliente al que se le concretó la venta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h2>2. Joints de datos<h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los DataFrames 'df_informacion_de_clientes' y 'df_simulaciones_clientes' en base a la columna 'rut'\n",
    "# El método 'how=\"left\"' asegura que todos los registros de 'df_informacion_de_clientes' se conserven,\n",
    "# incluso si no tienen coincidencia en 'df_simulaciones_clientes'.\n",
    "df_simulaciones_e_informacion_de_clientes = pd.merge(\n",
    "    df_informacion_de_clientes, \n",
    "    df_simulaciones_clientes, \n",
    "    on='rut', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Crear una nueva columna 'simulo' que indica si el cliente tiene un 'Monto_Simulado' o no\n",
    "# El método 'notna()' devuelve True para valores no nulos y False para nulos.\n",
    "# Luego, 'astype(int)' convierte estos valores booleanos en enteros (1 para True, 0 para False).\n",
    "df_simulaciones_e_informacion_de_clientes['simulo'] = df_simulaciones_e_informacion_de_clientes['Monto_Simulado'].notna().astype(int)\n",
    "\n",
    "# Eliminar columnas innecesarias 'Unnamed: 0_x' y 'Unnamed: 0_y' que podrían haber surgido durante la carga o manipulación de datos\n",
    "df_simulaciones_e_informacion_de_clientes.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_simulaciones_e_informacion_de_clientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los DataFrames 'df_simulaciones_e_informacion_de_clientes' y 'df_ventas' en base a las columnas 'rut' y 'fecha'\n",
    "# El método 'how=\"left\"' asegura que todos los registros de 'df_simulaciones_e_informacion_de_clientes' se conserven,\n",
    "# incluso si no tienen coincidencia en 'df_ventas'.\n",
    "df_simulaciones_e_informacion_de_clientes_ventas = pd.merge( \n",
    "    df_simulaciones_e_informacion_de_clientes, \n",
    "    df_ventas, \n",
    "    on=['rut', 'fecha'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Crear una nueva columna 'venta' que indica si existe una venta asociada al cliente y la fecha específica\n",
    "# El método 'notna()' verifica si hay un valor no nulo en la columna 'Unnamed: 0' (que indica presencia de una venta)\n",
    "# Luego, 'astype(int)' convierte estos valores booleanos en enteros (1 para True, 0 para False).\n",
    "df_simulaciones_e_informacion_de_clientes_ventas['venta'] = df_simulaciones_e_informacion_de_clientes_ventas['Unnamed: 0'].notna().astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los DataFrames 'df_simulaciones_e_informacion_de_clientes_ventas' y 'df_tratamiento' en base a las columnas 'rut' y 'fecha'\n",
    "# La unión se realiza con 'how=\"left\"' para conservar todos los registros de 'df_simulaciones_e_informacion_de_clientes_ventas'\n",
    "# incluso si no tienen coincidencia en 'df_tratamiento'.\n",
    "df_simulaciones_e_informacion_de_clientes_ventas_tratamiento = pd.merge( \n",
    "    df_simulaciones_e_informacion_de_clientes_ventas, \n",
    "    df_tratamiento, \n",
    "    on=['rut', 'fecha'], \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Crear una nueva columna 'mes' que extrae el mes y año de la columna 'fecha'\n",
    "# Primero se convierte 'fecha' al formato datetime, luego 'dt.to_period('M')' obtiene el periodo del mes/año.\n",
    "df_simulaciones_e_informacion_de_clientes_ventas_tratamiento['mes'] = pd.to_datetime(df_simulaciones_e_informacion_de_clientes_ventas_tratamiento['fecha']).dt.to_period('M')\n",
    "\n",
    "# Eliminar las columnas 'Unnamed: 0_x' y 'Unnamed: 0_y' ya que no aportan información relevante\n",
    "df_simulaciones_e_informacion_de_clientes_ventas_tratamiento.drop(columns=['Unnamed: 0_x', 'Unnamed: 0_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. CLUSTERING POR POLITICAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seteo de cluster. Aquí se definen las variables y sus cortes. La idea es que el algoritmo de RL haga sus acciones en esta sección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se definen que variables se utilizarán para crear el cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_informacion_de_clientes_procesados_cluster_definitivo = df_informacion_de_clientes[['rut', 'Elasticidad_Precios', 'Edad', 'Genero', 'Renta', 'Probabilidad_No_Pago']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui se definen en que partes y en cuantas partes se particionarán las variables escogidas anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame 'df_informacion_de_clientes_procesados_cluster_definitivo' para trabajar sin modificar el original\n",
    "df = df_informacion_de_clientes_procesados_cluster_definitivo.copy()\n",
    "\n",
    "# Clasificar la columna 'Probabilidad_No_Pago' en cinco categorías, asignando etiquetas según los valores de probabilidad\n",
    "# Cada categoría representa el nivel de confiabilidad en el pago: desde 'Muy buen pagador' hasta 'Muy mal pagador'.\n",
    "df['Categoria_Probabilidad_No_Pago'] = pd.cut(df['Probabilidad_No_Pago'], \n",
    "                                              bins=[-float('inf'), 0.0085402934056559, float('inf')],\n",
    "                                              labels=['Buen pagador', 'Mal pagador'])\n",
    "\n",
    "# Clasificar la columna 'Edad' en tres categorías: 'Joven', 'Adulto' y 'Adulto Mayor'\n",
    "# Cada categoría se define en función de rangos de edad especificados en 'bins'.\n",
    "df['Categoria_Edad'] = pd.cut(df['Edad'], \n",
    "                              bins=[-float('inf'), 31, float('inf')],\n",
    "                              labels=['Joven', 'Adulto'])\n",
    "\n",
    "# Crear un DataFrame único de 'rut' y 'Renta' eliminando duplicados, para calcular percentiles de renta\n",
    "df_unicos_renta = df[['rut', 'Renta']].drop_duplicates()\n",
    "\n",
    "# Clasificar la columna 'Percentil_Renta' en tres categorías: 'Renta Baja', 'Renta Media' y 'Renta Alta'\n",
    "# Los rangos de percentil especificados en 'bins' definen estas categorías.\n",
    "df_unicos_renta['Categoria_Renta'] = pd.cut(df_unicos_renta['Renta'], \n",
    "                                            bins=[-float('inf'), 705468.9044051456, 1382978.7928061879, float('inf')],\n",
    "                                            labels=['Renta Baja', 'Renta Media', 'Renta Alta'])\n",
    "\n",
    "# Incorporar la categoría de renta al DataFrame principal 'df' realizando una unión ('merge') en base a la columna 'rut'\n",
    "df = df.merge(df_unicos_renta[['rut', 'Categoria_Renta']], on='rut', how='left')\n",
    "\n",
    "# Mostrar el DataFrame resultante con las nuevas columnas creadas\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenar las variables especificadas en una nueva columna 'categoria_clusterizacion'\n",
    "# La columna resultante combinará varias categorías en una descripción detallada del perfil del cliente.\n",
    "# Convertimos cada columna a tipo string para asegurarnos de que los datos sean compatibles para la concatenación.\n",
    "\n",
    "df['categoria_clusterizacion'] = (\n",
    "    df['Elasticidad_Precios'].astype(str) + ' ' +              # Categoría de digitalización del cliente\n",
    "    df['Categoria_Edad'].astype(str) + ' de genero ' +       # Categoría de edad, seguida de la palabra \"de genero\"\n",
    "    df['Genero'].astype(str) + ' con una ' +                     # Género del cliente\n",
    "    df['Categoria_Renta'].astype(str) + 'que es un ' +                      # Categoría de renta\n",
    "    df['Categoria_Probabilidad_No_Pago'].astype(str)  # Categoría de probabilidad de no pago\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asignar un número único a cada entrada distinta en la columna 'categoria_clusterizacion'\n",
    "# Se convierte la columna a tipo 'category', lo cual facilita la asignación de códigos numéricos únicos.\n",
    "# 'cat.codes' asigna un código numérico único para cada valor único de 'categoria_clusterizacion'.\n",
    "df['categoria_clusterizacion_numerica'] = df['categoria_clusterizacion'].astype('category').cat.codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame con solo las columnas 'rut', 'categoria_clusterizacion' y 'categoria_clusterizacion_numerica'\n",
    "# Esta copia se almacena en el nuevo DataFrame 'asignacion_clusters', el cual contendrá únicamente la identificación del cliente (rut),\n",
    "# la descripción del perfil ('categoria_clusterizacion') y el código numérico asignado a cada perfil ('categoria_clusterizacion_numerica').\n",
    "asignacion_clusters = df[['rut', 'categoria_clusterizacion', 'categoria_clusterizacion_numerica']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Estimacion de curvas de elasticidad por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar una unión entre 'df_simulaciones_e_informacion_de_clientes_ventas' y 'asignacion_clusters' usando la columna 'rut' como clave\n",
    "# Esta unión ('merge') se realiza con 'how=\"left\"', lo que asegura que todos los registros de 'df_simulaciones_e_informacion_de_clientes_ventas' \n",
    "# se conserven, incluyendo aquellos sin coincidencia en 'asignacion_clusters'.\n",
    "# La finalidad es agregar la información de clusterización (categoría y código numérico) al DataFrame de simulaciones y ventas.\n",
    "df_estimar_elasticidad = pd.merge(df_simulaciones_e_informacion_de_clientes_ventas, asignacion_clusters, on='rut', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_elasticity_curve_with_histogram(tasas_grid, acceptance_probability, tasa_optima, df_cluster, cluster_num, output_folder):\n",
    "    \"\"\"\n",
    "    Grafica la curva de elasticidad con la tasa óptima marcada y un histograma\n",
    "    que muestra ventas y no ventas para cada precio, con mejoras en la visualización.\n",
    "    \n",
    "    Args:\n",
    "    - tasas_grid (numpy array): Valores de la tasa simulada (eje X).\n",
    "    - acceptance_probability (numpy array): Probabilidades de aceptación (eje Y).\n",
    "    - tasa_optima (float): Tasa óptima encontrada.\n",
    "    - df_cluster (DataFrame): Datos del cluster actual, incluyendo 'Tasa_Simulado' y 'venta'.\n",
    "    - cluster_num (int): Número del cluster para el título.\n",
    "    - output_folder (str): Ruta de la carpeta donde se guardará el gráfico.\n",
    "    \"\"\"\n",
    "    # Preparar datos para el histograma\n",
    "    ventas = df_cluster[df_cluster['venta'] == 1]['Tasa_Simulado']\n",
    "    no_ventas = df_cluster[df_cluster['venta'] == 0]['Tasa_Simulado']\n",
    "    \n",
    "    # Crear la figura\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Curva de elasticidad\n",
    "    ax1.plot(tasas_grid, acceptance_probability, label=\"Curva de Elasticidad\", color='blue', linewidth=2)\n",
    "    ax1.axvline(x=tasa_optima, color='green', linestyle='--', label=f\"Tasa Óptima: {tasa_optima:.2f}%\", zorder=10)\n",
    "    ax1.set_xlabel(\"Tasa Simulada (%)\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Probabilidad de Aceptación\", fontsize=12, color='black')\n",
    "    ax1.tick_params(axis='y', labelcolor='black')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.grid(True, linestyle='--', alpha=0.7)\n",
    "    ax1.legend(loc='upper left', bbox_to_anchor=(1.2, 1))  # Leyenda fuera de la gráfica\n",
    "    \n",
    "    # Histograma de ventas y no ventas (usar el segundo eje Y)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.hist([ventas, no_ventas], bins=20, color=['blue', 'red'], alpha=0.3, label=[\"Ventas\", \"No Ventas\"], stacked=True)\n",
    "    ax2.set_ylabel(\"Frecuencia de Ventas\", fontsize=12, color='black')\n",
    "    ax2.tick_params(axis='y', labelcolor='black')\n",
    "    ax2.legend(loc='upper left', bbox_to_anchor=(1.2, 0.85))  # Leyenda fuera de la gráfica\n",
    "\n",
    "    # Título\n",
    "    plt.title(f\"Curva de Elasticidad y Ventas por Tasa - Cluster {cluster_num}\", fontsize=14)\n",
    "    \n",
    "    # Ajustar diseño para evitar superposiciones\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar el gráfico\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    output_path = os.path.join(output_folder, f\"curva_elasticidad_cluster_{cluster_num}.png\")\n",
    "    plt.savefig(output_path)\n",
    "    print(f\"Gráfico guardado en: {output_path}\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Este código realiza un análisis de elasticidad de ingresos en función de clusters de clientes. Primero, agrupa los datos por clusters definidos a través de variables de segmentación y filtra solo los datos relevantes para cada cluster. Luego, para cada cluster, se ajusta un modelo de regresión logística para predecir la probabilidad de aceptación de una simulación de crédito en función de la tasa de interés. A partir de este modelo, se crea una cuadrícula de tasas para estimar la probabilidad de aceptación y calcular el revenue potencial de cada simulación, teniendo en cuenta el monto medio simulado, el plazo medio simulado y la probabilidad media de no pago del cluster. Posteriormente, se determina la tasa que maximiza el revenue esperado y se calcula el número esperado de créditos aceptados, junto con el número de clientes únicos en cada cluster. Finalmente, los resultados se agregan tanto en listas globales como en un nuevo DataFrame, y luego se integran en el DataFrame original df_estimar_elasticidad, lo que permite analizar el revenue esperado total y otros indicadores clave en cada cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_estimar_elasticidad(df_estimar_elasticidad):\n",
    "    # Inicializar listas para almacenar resultados globales de revenue, clientes, créditos y simulaciones\n",
    "    lista_revenue = []\n",
    "    lista_clientes = []\n",
    "    lista_creditos = []\n",
    "    lista_simulaciones = []\n",
    "\n",
    "    cluster_results = []  # Lista para almacenar resultados específicos de cada cluster\n",
    "\n",
    "    # Obtener los números únicos de cada cluster\n",
    "    cluster_numbers = df_estimar_elasticidad['categoria_clusterizacion_numerica'].unique()\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_folder = f\"hg_reglog_{timestamp}\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Iterar sobre cada cluster identificado por 'categoria_clusterizacion_numerica'\n",
    "    for cluster_num in cluster_numbers:\n",
    "        # Filtrar los datos correspondientes al cluster actual\n",
    "        df_cluster = df_estimar_elasticidad[df_estimar_elasticidad['categoria_clusterizacion_numerica'] == cluster_num]\n",
    "        \n",
    "        # Asegurarse de que existen datos para ambos casos: venta == 1 y venta == 0\n",
    "        if df_cluster.empty or df_cluster['venta'].isnull().all():\n",
    "            continue  # Saltar este cluster si no cumple con la condición\n",
    "        \n",
    "        # Remover filas donde 'venta' o 'Tasa_Simulado' son nulos o infinitos\n",
    "        df_cluster = df_cluster.replace([np.inf, -np.inf], np.nan)\n",
    "        df_cluster = df_cluster.dropna(subset=['venta', 'Tasa_Simulado', 'Plazo_Simulado', 'Monto_Simulado', 'Probabilidad_No_Pago'])\n",
    "        \n",
    "        # Saltar el cluster si no hay suficientes puntos de datos\n",
    "        if df_cluster.shape[0] < 10:\n",
    "            continue\n",
    "        \n",
    "        # Extraer las variables 'venta' (como variable dependiente) y 'Tasa_Simulado' (como predictor)\n",
    "        y = df_cluster['venta']\n",
    "        X = df_cluster[['Tasa_Simulado']]\n",
    "        \n",
    "        # Añadir un término constante para el intercepto\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # Remover filas con valores NaN o Inf en X o y\n",
    "        is_finite = np.isfinite(X).all(1) & np.isfinite(y)\n",
    "        X = X[is_finite]\n",
    "        y = y[is_finite]\n",
    "        \n",
    "        # Asegurarse de que después de remover NaN/Inf, todavía hay suficientes datos\n",
    "        if len(y) < 10:\n",
    "            continue\n",
    "        \n",
    "        # Ajustar el modelo de regresión logística\n",
    "        logit_model = sm.Logit(y, X)\n",
    "        try:\n",
    "            result = logit_model.fit(disp=0)\n",
    "        except:\n",
    "            continue  # Saltar el cluster si el modelo no converge\n",
    "        \n",
    "        # Crear una cuadrícula de valores de 'Tasa_Simulado' para predicciones\n",
    "        tasa_min = df_cluster['Tasa_Simulado'].min()\n",
    "        tasa_max = df_cluster['Tasa_Simulado'].max()\n",
    "        tasas_grid = np.linspace(tasa_min, tasa_max, 1000)\n",
    "        \n",
    "        # Predecir la probabilidad de aceptación usando el modelo ajustado\n",
    "        X_grid = sm.add_constant(tasas_grid)\n",
    "        acceptance_probability = result.predict(X_grid)\n",
    "        \n",
    "        # Asegurar que las probabilidades están en el rango [0, 1]\n",
    "        acceptance_probability = np.clip(acceptance_probability, 0, 1)\n",
    "\n",
    "        # Calcular valores medios necesarios para el cálculo de revenue\n",
    "        n = df_cluster['Plazo_Simulado'].mean()\n",
    "        vp = df_cluster['Monto_Simulado'].mean()\n",
    "        pnp = df_cluster['Probabilidad_No_Pago'].mean()\n",
    "        data = {\n",
    "            'Plazo_Simulado_medio': n, \n",
    "            'Monto_Simulado_medio': vp, \n",
    "            'Probabilidad_No_Pago_media': pnp\n",
    "        }\n",
    "        \n",
    "        # Calcular el revenue potencial\n",
    "        i = tasas_grid / 100  # Convertir a decimal\n",
    "        one_plus_i_pow_n = np.power(1 + i, n)\n",
    "        annuity_factor = (i * one_plus_i_pow_n) / (one_plus_i_pow_n - 1)\n",
    "        revenue = (n * vp * annuity_factor) - vp\n",
    "        potential_revenue = revenue * (1 - pnp)\n",
    "        \n",
    "        # Calcular el promedio de simulaciones por fecha\n",
    "        df_cluster_simulaciones_1 = df_cluster[df_cluster['simulo'] == 1]\n",
    "        num_dates = df_cluster_simulaciones_1['fecha'].nunique()\n",
    "        total_simulaciones = df_cluster_simulaciones_1['simulo'].sum()\n",
    "        simulaciones_medias = total_simulaciones / num_dates if num_dates else 0\n",
    "        \n",
    "        # Saltar el cluster si no hay simulaciones\n",
    "        if simulaciones_medias == 0:\n",
    "            continue\n",
    "        \n",
    "        # Calcular el revenue esperado\n",
    "        expected_revenue = acceptance_probability * potential_revenue * simulaciones_medias\n",
    "        \n",
    "        # Encontrar la tasa que maximiza el revenue esperado\n",
    "        idx_max = np.argmax(expected_revenue)\n",
    "        max_price = tasas_grid[idx_max]\n",
    "        max_expected_revenue = expected_revenue[idx_max]\n",
    "        \n",
    "        # Probabilidad de aceptación en la tasa óptima\n",
    "        prob_aceptacion_optima = acceptance_probability[idx_max]\n",
    "        \n",
    "        # Llamar a la función para graficar la curva y el histograma\n",
    "        # plot_elasticity_curve_with_histogram(\n",
    "        #     tasas_grid, \n",
    "        #     acceptance_probability, \n",
    "        #     max_price, \n",
    "        #     df_cluster,  # Pasamos todo el DataFrame del cluster actual\n",
    "        #     cluster_num, \n",
    "        #     output_folder\n",
    "        # )\n",
    "\n",
    "        # Número esperado de créditos aceptados\n",
    "        num_creditos_aceptados = round(prob_aceptacion_optima * simulaciones_medias)\n",
    "        \n",
    "        # Número de clientes únicos en el cluster\n",
    "        num_clients = df_cluster['rut'].nunique()\n",
    "        \n",
    "        # Imprimir resultados para cada cluster\n",
    "        print(f'Cluster {cluster_num}:')\n",
    "        print(f'- Precio Máx. Revenue Esperado = {max_price:.2f}%')\n",
    "        print(f'- Revenue Esperado Máximo = {max_expected_revenue:,.2f}')\n",
    "        print(f'- Número de clientes en el cluster = {num_clients}')\n",
    "        print(f'- Número de simulaciones en el cluster = {simulaciones_medias:.2f}')\n",
    "        print(f'- Probabilidad de aceptación en el precio óptimo = {prob_aceptacion_optima:.4f}')\n",
    "        print(f'- Número esperado de créditos aceptados = {num_creditos_aceptados}')\n",
    "        print(f'- Monto medio simulado = {data[\"Monto_Simulado_medio\"]:,.2f}')\n",
    "        print(f'- Plazo medio simulado = {data[\"Plazo_Simulado_medio\"]:,.2f}')\n",
    "        print(f'- Probabilidad de no pago media = {data[\"Probabilidad_No_Pago_media\"]:.4f}\\n')\n",
    "\n",
    "        # Agregar resultados a las listas globales\n",
    "        lista_clientes.append(num_clients)\n",
    "        lista_revenue.append(max_expected_revenue)\n",
    "        lista_creditos.append(num_creditos_aceptados)\n",
    "        lista_simulaciones.append(simulaciones_medias)\n",
    "        \n",
    "        # Almacenar resultados por cluster en cluster_results\n",
    "        cluster_results.append({\n",
    "            'categoria_clusterizacion_numerica': cluster_num,\n",
    "            'tasa_optima': max_price,\n",
    "            'probabilidad_aceptacion_optima': prob_aceptacion_optima,\n",
    "            'revenue_esperado_maximo': max_expected_revenue,\n",
    "            'numero_clientes': num_clients,\n",
    "            'numero_simulaciones_medias': simulaciones_medias,\n",
    "            'numero_creditos_esperados': num_creditos_aceptados,\n",
    "            'monto_medio_simulado': data[\"Monto_Simulado_medio\"],\n",
    "            'plazo_medio_simulado': data[\"Plazo_Simulado_medio\"],\n",
    "            'probabilidad_no_pago_media': data[\"Probabilidad_No_Pago_media\"]\n",
    "        })\n",
    "\n",
    "    # Imprimir resultados globales\n",
    "    total_revenue = sum(lista_revenue)\n",
    "    total_clientes = sum(lista_clientes)\n",
    "    total_simulaciones = sum(lista_simulaciones)\n",
    "    total_creditos = sum(lista_creditos)\n",
    "\n",
    "    print(f\"El revenue total esperado es: {total_revenue:,.2f} con un total de {total_clientes} clientes, \"\n",
    "        f\"{total_simulaciones:,.2f} simulaciones, y {total_creditos} créditos.\")\n",
    "\n",
    "    # Crear un DataFrame a partir de cluster_results\n",
    "    df_cluster_results = pd.DataFrame(cluster_results)\n",
    "\n",
    "    # Incorporar los resultados por cluster de 'df_cluster_results' a 'df_estimar_elasticidad'\n",
    "    df_estimar_elasticidad = df_estimar_elasticidad.merge(\n",
    "        df_cluster_results[['categoria_clusterizacion_numerica', 'tasa_optima', 'probabilidad_aceptacion_optima']],\n",
    "        on='categoria_clusterizacion_numerica', \n",
    "        how='left'\n",
    "    )\n",
    "    return {'df_estimar_elasticidad': df_estimar_elasticidad, 'total_revenue': total_revenue, 'total_clientes': total_clientes, 'total_simulaciones': total_simulaciones, 'total_creditos': total_creditos}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estimar_elasticidad = function_estimar_elasticidad(df_estimar_elasticidad)['df_estimar_elasticidad']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Estimacion de respuesta a tratamiento por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unir los DataFrames 'df_tratamiento' y 'df_simulaciones_clientes' usando las columnas 'rut' y 'fecha' como claves\n",
    "# La unión se realiza con 'how=\"left\"', lo cual asegura que todos los registros de 'df_tratamiento' se conserven,\n",
    "# incluyendo aquellos sin coincidencia en 'df_simulaciones_clientes'.\n",
    "# Esta operación permite combinar la información de tratamiento con los datos de simulaciones de clientes.\n",
    "df_simulaciones_info = pd.merge(df_tratamiento, df_simulaciones_clientes, on=['rut', 'fecha'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar operaciones de cadenas vectorizadas para crear la columna 'Tratamiento'\n",
    "# Esta columna concatenará información sobre el ejecutivo asignado y el número de correos enviados.\n",
    "# Se convierte 'asg_ejec' a string para poder concatenar, y 'n_correos' se convierte primero a entero y luego a string.\n",
    "# El formato final es: \"Ejecutivo=<valor_asg_ejec>, Correos=<valor_n_correos>\"\n",
    "df_simulaciones_info['Tratamiento'] = (\n",
    "    'Ejecutivo=' + df_simulaciones_info['asg_ejec'].astype(str) +\n",
    "    ', Correos=' + df_simulaciones_info['n_correos'].astype(int).astype(str)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer el mes y año de la columna 'fecha' y crear una nueva columna 'mes' en formato de periodo mensual\n",
    "# Se convierte 'fecha' al formato datetime y luego se usa 'dt.to_period('M')' para obtener el mes/año.\n",
    "df_simulaciones_info['mes'] = pd.to_datetime(df_simulaciones_info['fecha']).dt.to_period('M')\n",
    "\n",
    "# Filtrar y mostrar las filas donde 'rut' es igual a 1\n",
    "# Este filtro permite observar los registros específicos del cliente con 'rut' igual a 1, \n",
    "# lo cual es útil para verificar datos o analizar un cliente en particular.\n",
    "df_simulaciones_info[df_simulaciones_info['rut'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una nueva columna 'simulo' para indicar si el cliente tiene un registro de simulación\n",
    "# La columna 'Unnamed: 0_y' se utiliza para verificar si hay un valor no nulo, lo que implica que hay una simulación.\n",
    "# 'notna()' devuelve True para valores no nulos y False para valores nulos; luego, 'astype(int)' convierte estos valores a 1 (True) o 0 (False).\n",
    "df_simulaciones_info['simulo'] = df_simulaciones_info['Unnamed: 0_y'].notna().astype(int)\n",
    "\n",
    "# Filtrar y mostrar las filas donde 'rut' es igual a 1\n",
    "# Este filtro permite observar los registros específicos del cliente con 'rut' igual a 1, \n",
    "# útil para verificar si la columna 'simulo' refleja correctamente la presencia de simulaciones para este cliente.\n",
    "df_simulaciones_info[df_simulaciones_info['rut'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una copia del DataFrame 'df_estimar_elasticidad' con solo las columnas especificadas\n",
    "# 'df1' contiene las columnas 'rut', 'categoria_clusterizacion_numerica', 'tasa_optima' y 'probabilidad_aceptacion_optima'.\n",
    "# Esta copia es útil para trabajar con los datos de elasticidad y clusterización sin modificar el DataFrame original.\n",
    "df1 = df_estimar_elasticidad[['rut', 'categoria_clusterizacion_numerica', 'tasa_optima', 'probabilidad_aceptacion_optima']].copy()\n",
    "\n",
    "# Crear una copia del DataFrame 'df_simulaciones_info' con solo las columnas especificadas\n",
    "# 'df2' contiene las columnas 'rut', 'mes', 'Tratamiento' y 'simulo'.\n",
    "# Esta copia es útil para trabajar con los datos de tratamiento y simulación en un conjunto de datos reducido.\n",
    "df2 = df_simulaciones_info[['rut', 'mes', 'Tratamiento', 'simulo']].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_estimar_respuesta_a_tratamiento(df_estimar_elasticidad, df_simulaciones_info): #df1 es df_estimar_elasticidad y df2 es df_simulaciones_info\n",
    "    # Paso 1: Preparación de datos y mapeo de clusters\n",
    "    # Eliminar duplicados en 'df1' para tener un valor único de 'categoria_clusterizacion_numerica' por cada 'rut'.\n",
    "    df_estimar_elasticidad_unique = df_estimar_elasticidad.drop_duplicates(subset='rut')\n",
    "\n",
    "    # Crear un mapeo de 'rut' a 'categoria_clusterizacion_numerica' para asociar cada cliente a su cluster numérico.\n",
    "    rut_cluster_map = df_estimar_elasticidad_unique.set_index('rut')['categoria_clusterizacion_numerica']\n",
    "\n",
    "    # Mapear la categoría de cluster a cada 'rut' en 'df2' usando el mapeo creado\n",
    "    df_simulaciones_info['categoria_clusterizacion_numerica'] = df_simulaciones_info['rut'].map(rut_cluster_map)\n",
    "\n",
    "    # Eliminar filas donde 'categoria_clusterizacion_numerica' es nulo, es decir, aquellos 'rut' sin mapeo de cluster.\n",
    "    df_simulaciones_info = df_simulaciones_info.dropna(subset=['categoria_clusterizacion_numerica'])\n",
    "\n",
    "    # Conversión de tipos de datos\n",
    "    # Convertir 'categoria_clusterizacion_numerica' a entero para garantizar un tipo de dato consistente.\n",
    "    df_simulaciones_info['categoria_clusterizacion_numerica'] = df_simulaciones_info['categoria_clusterizacion_numerica'].astype(int)\n",
    "\n",
    "    # Convertir 'simulo' a numérico, reemplazando valores nulos por 0 y asegurando que sea un tipo de dato entero.\n",
    "    df_simulaciones_info['simulo'] = pd.to_numeric(df_simulaciones_info['simulo'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    # Convertir 'Tratamiento' a tipo de categoría para optimizar espacio y realizar operaciones categóricas.\n",
    "    df_simulaciones_info['Tratamiento'] = df_simulaciones_info['Tratamiento'].astype('category')\n",
    "\n",
    "    # Paso 2: Calcular el caso total (entradas por tratamiento sin importar el valor de 'simulo')\n",
    "    # Agrupar por 'categoria_clusterizacion_numerica' y 'Tratamiento' para contar el número total de registros en cada combinación.\n",
    "    total_entries_per_cluster_treatment = df_simulaciones_info.groupby(['categoria_clusterizacion_numerica', 'Tratamiento']).size().reset_index(name='caso_total')\n",
    "\n",
    "    # Paso 3: Calcular el caso favorable (entradas por tratamiento cuando 'simulo' == 1)\n",
    "    # Filtrar filas donde 'simulo' es 1 (clientes que realizaron una simulación)\n",
    "    df_simulations = df_simulaciones_info[df_simulaciones_info['simulo'] == 1]\n",
    "\n",
    "    # Agrupar por 'categoria_clusterizacion_numerica' y 'Tratamiento' para contar el número de registros favorables (simulaciones).\n",
    "    favorable_entries_per_cluster_treatment = df_simulations.groupby(['categoria_clusterizacion_numerica', 'Tratamiento']).size().reset_index(name='caso_favorable')\n",
    "\n",
    "    # Paso 4: Calcular la probabilidad de simulación como caso favorable / caso total\n",
    "    # Realizar un merge entre 'total_entries_per_cluster_treatment' y 'favorable_entries_per_cluster_treatment' en las columnas de cluster y tratamiento.\n",
    "    df_probabilities = total_entries_per_cluster_treatment.merge(\n",
    "        favorable_entries_per_cluster_treatment,\n",
    "        on=['categoria_clusterizacion_numerica', 'Tratamiento'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Llenar valores nulos en 'caso_favorable' con 0, asegurando que solo las columnas numéricas estén afectadas.\n",
    "    df_probabilities['caso_favorable'] = df_probabilities['caso_favorable'].fillna(0).astype(int)\n",
    "\n",
    "    # Asegurar que 'caso_total' sea de tipo entero para evitar inconsistencias en los conteos.\n",
    "    df_probabilities['caso_total'] = df_probabilities['caso_total'].astype(int)\n",
    "\n",
    "    # Calcular la probabilidad de simulación como el cociente entre 'caso_favorable' y 'caso_total'.\n",
    "    df_probabilities['probabilidad_simular'] = df_probabilities['caso_favorable'] / df_probabilities['caso_total']\n",
    "\n",
    "    # Organizar las columnas del DataFrame resultante para facilitar su análisis.\n",
    "    df_probabilities = df_probabilities[[\n",
    "        'categoria_clusterizacion_numerica',\n",
    "        'Tratamiento',\n",
    "        'probabilidad_simular',\n",
    "        'caso_favorable',\n",
    "        'caso_total'\n",
    "    ]]\n",
    "\n",
    "    # Mostrar el DataFrame resultante con la probabilidad de simulación calculada para cada combinación de cluster y tratamiento.\n",
    "    return df_probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probabilities = function_estimar_respuesta_a_tratamiento(df1, df2)\n",
    "df_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el DataFrame 'df_estimar_elasticidad' para revisar su contenido antes de realizar cálculos adicionales\n",
    "df_estimar_elasticidad\n",
    "\n",
    "# Calcular el valor promedio de 'Monto_Simulado' para cada 'categoria_clusterizacion_numerica'\n",
    "# Usamos 'groupby' para agrupar por 'categoria_clusterizacion_numerica' y 'transform(\"mean\")' para calcular el promedio.\n",
    "# Luego, 'transform' asigna este valor promedio a cada fila dentro de su grupo, creando una columna 'Monto_Simulado_mean' con estos promedios.\n",
    "df_estimar_elasticidad['Monto_Simulado_mean'] = df_estimar_elasticidad.groupby('categoria_clusterizacion_numerica')['Monto_Simulado'].transform('mean')\n",
    "\n",
    "# Calcular el valor promedio de 'Plazo_Simulado' para cada 'categoria_clusterizacion_numerica'\n",
    "# Similar al cálculo anterior, 'groupby' agrupa los datos por 'categoria_clusterizacion_numerica', y 'transform(\"mean\")' calcula el promedio.\n",
    "# Se asigna el promedio resultante a cada fila dentro del grupo en la nueva columna 'Plazo_Simulado_mean'.\n",
    "df_estimar_elasticidad['Plazo_Simulado_mean'] = df_estimar_elasticidad.groupby('categoria_clusterizacion_numerica')['Plazo_Simulado'].transform('mean')\n",
    "df_estimar_elasticidad['Plazo_Simulado_min'] = df_estimar_elasticidad.groupby('categoria_clusterizacion_numerica')['Plazo_Simulado'].transform('min')\n",
    "df_estimar_elasticidad['Plazo_Simulado_max'] = df_estimar_elasticidad.groupby('categoria_clusterizacion_numerica')['Plazo_Simulado'].transform('max')\n",
    "df_estimar_elasticidad['Plazo_Simulado_mode'] = df_estimar_elasticidad.groupby('categoria_clusterizacion_numerica')['Plazo_Simulado'].transform(lambda x: x.mode().iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas necesarias del DataFrame 'df_estimar_elasticidad' para reducir su tamaño\n",
    "# 'df_estimar_elasticidad_small' contiene las columnas esenciales para el análisis:\n",
    "# 'categoria_clusterizacion_numerica', 'rut', 'tasa_optima', 'probabilidad_aceptacion_optima', 'Probabilidad_No_Pago',\n",
    "# 'Monto_Simulado_mean', y 'Plazo_Simulado_mean'.\n",
    "df_estimar_elasticidad_small = df_estimar_elasticidad[['categoria_clusterizacion_numerica', 'rut', 'tasa_optima', 'probabilidad_aceptacion_optima', 'Probabilidad_No_Pago', \n",
    "                                                       'Monto_Simulado_mean',\n",
    "                                                       'Plazo_Simulado_mean', 'Plazo_Simulado_min', 'Plazo_Simulado_max', 'Plazo_Simulado_mode']]\n",
    "\n",
    "# Seleccionar solo las columnas necesarias del DataFrame 'df_probabilities' para reducir su tamaño\n",
    "# 'df_probabilities_small' contiene las columnas 'categoria_clusterizacion_numerica', 'probabilidad_simular', y 'Tratamiento'.\n",
    "df_probabilities_small = df_probabilities[['categoria_clusterizacion_numerica', 'probabilidad_simular', 'Tratamiento']]\n",
    "\n",
    "# Realizar un merge entre 'df_estimar_elasticidad_small' y 'df_probabilities_small' usando 'categoria_clusterizacion_numerica' como clave\n",
    "# Esta unión ('how=\"left\"') mantiene todas las filas de 'df_estimar_elasticidad_small' y añade la información de 'df_probabilities_small'\n",
    "# cuando hay coincidencias en 'categoria_clusterizacion_numerica'. El resultado se guarda en 'df_asignacion_de_tratamientos'.\n",
    "df_asignacion_de_tratamientos = pd.merge(df_estimar_elasticidad_small, df_probabilities_small, on='categoria_clusterizacion_numerica', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un nombre de carpeta con una marca de tiempo actual\n",
    "# 'strftime' genera la fecha y hora actual en el formato \"YYYYMMDD_HHMMSS\".\n",
    "# Esto se usa para crear una carpeta única 'folder_name' donde se guardarán los archivos.\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_name = f\"cluster_data_{timestamp}\"\n",
    "os.makedirs(folder_name, exist_ok=True)  # Crear la carpeta; 'exist_ok=True' evita errores si ya existe.\n",
    "\n",
    "# Guardar información de los clusters\n",
    "# Seleccionar las columnas relevantes sobre cada cluster desde 'df_estimar_elasticidad_small' y eliminar duplicados.\n",
    "# El DataFrame 'df_cluster_info' contiene datos únicos de cada cluster como el monto y plazo medio simulado, la probabilidad de aceptación óptima y la tasa óptima.\n",
    "df_cluster_info = df_estimar_elasticidad_small[['categoria_clusterizacion_numerica', 'probabilidad_aceptacion_optima', 'tasa_optima',\n",
    "                                                'Monto_Simulado_mean',\n",
    "                                                'Plazo_Simulado_mean', 'Plazo_Simulado_min', 'Plazo_Simulado_max', 'Plazo_Simulado_mode']].drop_duplicates()\n",
    "df_cluster_info.to_csv(f\"{folder_name}/cluster_info.csv\", index=False)\n",
    "\n",
    "# Guardar las probabilidades y tratamiento\n",
    "# Seleccionar columnas relevantes de 'df_probabilities_small' para almacenar la probabilidad de simulación y tratamiento asignado para cada cluster.\n",
    "# 'df_probabilities_treatment' contiene esta información única por cada combinación de cluster y tratamiento.\n",
    "df_probabilities_treatment = df_probabilities_small[['categoria_clusterizacion_numerica', 'probabilidad_simular', 'Tratamiento']].drop_duplicates()\n",
    "df_probabilities_treatment.to_csv(f\"{folder_name}/probabilities_treatment.csv\", index=False)\n",
    "\n",
    "# Guardar información del RUT\n",
    "# Seleccionar columnas clave sobre cada cliente ('rut') desde 'df_estimar_elasticidad_small' y eliminar duplicados.\n",
    "# 'df_rut_info' contiene el 'rut', la categoría de cluster y la probabilidad de no pago para cada cliente, sin registros duplicados.\n",
    "df_rut_info = df_estimar_elasticidad_small[['rut', 'categoria_clusterizacion_numerica', 'Probabilidad_No_Pago']].drop_duplicates()\n",
    "df_rut_info.to_csv(f\"{folder_name}/rut_info.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Modelo de asignacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo de asignacion que itera por cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelo_optimizacion(df_probabilities_treatment, df_rut_info, df_cluster_info, costo_sms, ejecutivos):\n",
    "    # -------------------------------\n",
    "    # Procesamiento y preprocesamiento de datos\n",
    "    # -------------------------------\n",
    "\n",
    "    # Definir la carpeta base y el mapeo de tratamientos\n",
    "    tratamiento_map = {  # Mapeo de los tratamientos específicos a identificadores numéricos\n",
    "        \"Ejecutivo=0, Correos=0\": 1, \"Ejecutivo=0, Correos=1\": 2,\n",
    "        \"Ejecutivo=0, Correos=2\": 3, \"Ejecutivo=0, Correos=3\": 4,\n",
    "        \"Ejecutivo=0, Correos=4\": 5, \"Ejecutivo=1, Correos=0\": 6,\n",
    "        \"Ejecutivo=1, Correos=1\": 7, \"Ejecutivo=1, Correos=2\": 8\n",
    "    }\n",
    "\n",
    "    # Parámetros\n",
    "    costosms = costo_sms # Costo de cada mensaje SMS\n",
    "    capacidad_ejecutivos = ejecutivos  # Capacidad máxima en términos de tiempo de los ejecutivos\n",
    "\n",
    "    # Paso 1: Cargar y mapear 'tratamiento_id' en los datos de probabilidades\n",
    "    print(\"Loading and processing probabilities data...\")\n",
    "    df_probabilities = df_probabilities_treatment\n",
    "    df_probabilities['tratamiento_id'] = df_probabilities['Tratamiento'].map(tratamiento_map)\n",
    "\n",
    "    # Paso 2: Crear lista de tratamientos y combinar con rut_info\n",
    "    print(\"Merging probabilities with rut_info...\")\n",
    "    df_probabilities['tratamientos'] = df_probabilities[['probabilidad_simular', 'tratamiento_id']].values.tolist()\n",
    "    grouped_prob = df_probabilities.groupby('categoria_clusterizacion_numerica')['tratamientos'].apply(list).reset_index()\n",
    "\n",
    "\n",
    "    df_rut_info1 = df_rut_info.merge(grouped_prob, on='categoria_clusterizacion_numerica', how='left')\n",
    "\n",
    "\n",
    "    # Paso 3: Combinar rut_info con cluster_info\n",
    "    print(\"Merging rut_info with cluster_info...\")\n",
    "    df_cluster_info = df_cluster_info\n",
    "    df_rut_info2 = df_rut_info1.merge(df_cluster_info, on='categoria_clusterizacion_numerica', how='left')\n",
    "\n",
    "    # Paso 3.5: Agrupar información por cluster en 'rut_info'\n",
    "    # Agrupar por 'categoria_clusterizacion_numerica' y agregar según lo especificado\n",
    "    df_grouped = df_rut_info2.groupby('categoria_clusterizacion_numerica').agg({\n",
    "        'Probabilidad_No_Pago': 'mean',  # Promedio de probabilidad de no pago\n",
    "        'tratamientos': lambda x: list(x),  # Lista de opciones de tratamiento únicas en cada cluster\n",
    "        'Monto_Simulado_mean': 'mean',\n",
    "        'Plazo_Simulado_mean': 'mean',\n",
    "        'probabilidad_aceptacion_optima': 'mean',\n",
    "        'tasa_optima': 'mean',\n",
    "        'rut': 'count'  # Conteo del número de clientes ('rut') en cada cluster\n",
    "    }).rename(columns={'rut': 'n_clientes'}).reset_index()\n",
    "\n",
    "    # Paso 4: Calcular 'RC' (Revenue calculado)\n",
    "    print(\"Calculating RC...\")\n",
    "    df_grouped['tasa_optima'] /= 100  # Convertir tasa óptima a decimal\n",
    "    df_grouped['RC'] = (\n",
    "        (df_grouped['Plazo_Simulado_mean'] * df_grouped['Monto_Simulado_mean'] * df_grouped['tasa_optima'] *\n",
    "        ((1 + df_grouped['tasa_optima']) ** df_grouped['Plazo_Simulado_mean'])) /\n",
    "        (((1 + df_grouped['tasa_optima']) ** df_grouped['Plazo_Simulado_mean']) - 1)\n",
    "    ) - df_grouped['Monto_Simulado_mean']\n",
    "\n",
    "    # -------------------------------\n",
    "    # Preparación de datos para optimización\n",
    "    # -------------------------------\n",
    "\n",
    "    # Convertir 'tratamientos' a un arreglo de numpy para mejorar la indexación\n",
    "    # Desarrollar y preparar 'tratamientos' para indexación adecuada\n",
    "    profits = np.array([\n",
    "        [\n",
    "            row['n_clientes'] * (row['RC'] * (1 - row['Probabilidad_No_Pago']) * row['probabilidad_aceptacion_optima'] * row['tratamientos'][0][t][0]) - \n",
    "            (row['tratamientos'][0][t][1] * costosms)\n",
    "            for t in range(8)\n",
    "        ]\n",
    "        for _, row in df_grouped.iterrows()\n",
    "    ])\n",
    "\n",
    "    # Inicializar el modelo de optimización\n",
    "    model = Model(\"Maximizar_Ganancias\")\n",
    "    model.ModelSense = GRB.MAXIMIZE\n",
    "\n",
    "    # Crear variables de decisión y definir el objetivo\n",
    "    n_clients, n_treatments = profits.shape\n",
    "    variables = {}\n",
    "\n",
    "    for i in range(n_clients):\n",
    "        variables[i] = {}\n",
    "        for t in range(n_treatments):\n",
    "            if profits[i, t] > 0:\n",
    "                variables[i][t] = model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{t}\")\n",
    "\n",
    "    model.setObjective(\n",
    "        quicksum(variables[i][t] * profits[i, t] for i in variables for t in variables[i])\n",
    "    )\n",
    "\n",
    "    # Restricción: Cada cliente recibe exactamente un tratamiento\n",
    "    for i in variables:\n",
    "        model.addConstr(quicksum(variables[i].values()) == 1, name=f\"OneTreatmentPerClient_{i}\")\n",
    "\n",
    "    # Restricción de capacidad para ejecutivos\n",
    "    model.addConstr(\n",
    "        quicksum(variables[i][t] * df_grouped.loc[i, 'n_clientes'] for i in variables for t in variables[i] if t in [5, 6, 7]) <= capacidad_ejecutivos,\n",
    "        name=\"CapacityConstraint\"\n",
    "    )\n",
    "\n",
    "    # Consistencia de cluster: los clientes dentro del mismo cluster deben recibir el mismo tratamiento\n",
    "    clusters = df_grouped.groupby(\"categoria_clusterizacion_numerica\").indices\n",
    "    for cluster_id, indices_cluster in clusters.items():\n",
    "        indices_list = list(indices_cluster)\n",
    "        leader_index = indices_list[0]\n",
    "        for t in variables[leader_index]:\n",
    "            leader_var = variables[leader_index][t]\n",
    "            for i in indices_list[1:]:\n",
    "                if t in variables[i]:\n",
    "                    model.addConstr(variables[i][t] == leader_var, name=f\"ClusterConsistency_{cluster_id}_{t}\")\n",
    "\n",
    "    # Optimizar el modelo\n",
    "    model.optimize()\n",
    "\n",
    "    # Verificar si la optimización fue exitosa\n",
    "    if model.Status == GRB.OPTIMAL:\n",
    "        # -------------------------------\n",
    "        # Extracción y visualización de resultados\n",
    "        # -------------------------------\n",
    "\n",
    "        print(\"Extracting results...\")\n",
    "\n",
    "        # Asignar tratamientos por cluster basado en los resultados de la optimización\n",
    "        resultados_por_cluster = {}\n",
    "        for cluster_id, indices_cluster in clusters.items():\n",
    "            leader_index = list(indices_cluster)[0]\n",
    "            for t in variables[leader_index]:\n",
    "                if variables[leader_index][t].X > 0.5:\n",
    "                    resultados_por_cluster[cluster_id] = t + 1\n",
    "                    break\n",
    "\n",
    "        # Calcular las ganancias totales\n",
    "        ganancias_totales = model.ObjVal\n",
    "\n",
    "        # Mostrar resultados\n",
    "        print(\"\\nTratamientos asignados por cluster:\")\n",
    "        for cluster_id, tratamiento in resultados_por_cluster.items():\n",
    "            print(f\"Cluster {cluster_id}: Tratamiento {tratamiento}\")\n",
    "\n",
    "        print(f\"\\nGanancias totales: {ganancias_totales:.2f}\")\n",
    "\n",
    "        # Calcular el número de ejecutivos usados y restantes\n",
    "        executives_used = sum(\n",
    "            df_grouped.loc[i, 'n_clientes'] for i in variables for t in variables[i]\n",
    "            if t in [5, 6, 7] and variables[i][t].X > 0.5\n",
    "        )\n",
    "        executives_remaining = capacidad_ejecutivos - executives_used\n",
    "\n",
    "        # Mostrar resumen de uso de ejecutivos\n",
    "        print(f\"\\nExecutives used: {executives_used}\")\n",
    "        print(f\"Executives remaining: {executives_remaining}\")\n",
    "    else:\n",
    "        print(\"Optimization did not reach an optimal solution.\")\n",
    "    print(\"Optimization complete.\")\n",
    "    return ganancias_totales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Procesamiento y preprocesamiento de datos\n",
    "# -------------------------------\n",
    "# Definir la carpeta base y el mapeo de tratamientos\n",
    "tratamiento_map = {  # Mapeo de los tratamientos específicos a identificadores numéricos\n",
    "    \"Ejecutivo=0, Correos=0\": 1, \"Ejecutivo=0, Correos=1\": 2,\n",
    "    \"Ejecutivo=0, Correos=2\": 3, \"Ejecutivo=0, Correos=3\": 4,\n",
    "    \"Ejecutivo=0, Correos=4\": 5, \"Ejecutivo=1, Correos=0\": 6,\n",
    "    \"Ejecutivo=1, Correos=1\": 7, \"Ejecutivo=1, Correos=2\": 8\n",
    "}\n",
    "# Parámetros\n",
    "costosms = 100 # Costo de cada mensaje SMS\n",
    "capacidad_ejecutivos = 205000  # Capacidad máxima en términos de tiempo de los ejecutivos\n",
    "# Paso 1: Cargar y mapear 'tratamiento_id' en los datos de probabilidades\n",
    "print(\"Loading and processing probabilities data...\")\n",
    "df_probabilities = df_probabilities_treatment\n",
    "df_probabilities['tratamiento_id'] = df_probabilities['Tratamiento'].map(tratamiento_map)\n",
    "# Paso 2: Crear lista de tratamientos y combinar con rut_info\n",
    "print(\"Merging probabilities with rut_info...\")\n",
    "df_probabilities['tratamientos'] = df_probabilities[['probabilidad_simular', 'tratamiento_id']].values.tolist()\n",
    "grouped_prob = df_probabilities.groupby('categoria_clusterizacion_numerica')['tratamientos'].apply(list).reset_index()\n",
    "df_rut_info1 = df_rut_info.merge(grouped_prob, on='categoria_clusterizacion_numerica', how='left')\n",
    "# Paso 3: Combinar rut_info con cluster_info\n",
    "print(\"Merging rut_info with cluster_info...\")\n",
    "df_cluster_info = df_cluster_info\n",
    "df_rut_info2 = df_rut_info1.merge(df_cluster_info, on='categoria_clusterizacion_numerica', how='left')\n",
    "# Paso 3.5: Agrupar información por cluster en 'rut_info'\n",
    "# Agrupar por 'categoria_clusterizacion_numerica' y agregar según lo especificado\n",
    "df_grouped = df_rut_info2.groupby('categoria_clusterizacion_numerica').agg({\n",
    "    'Probabilidad_No_Pago': 'mean',  # Promedio de probabilidad de no pago\n",
    "    'tratamientos': lambda x: list(x),  # Lista de opciones de tratamiento únicas en cada cluster\n",
    "    'Monto_Simulado_mean': 'mean',\n",
    "    'Plazo_Simulado_mean': 'mean',\n",
    "    'probabilidad_aceptacion_optima': 'mean',\n",
    "    'tasa_optima': 'mean',\n",
    "    'rut': 'count'  # Conteo del número de clientes ('rut') en cada cluster\n",
    "}).rename(columns={'rut': 'n_clientes'}).reset_index()\n",
    "# Paso 4: Calcular 'RC' (Revenue calculado)\n",
    "print(\"Calculating RC...\")\n",
    "df_grouped['tasa_optima'] /= 100  # Convertir tasa óptima a decimal\n",
    "df_grouped['RC'] = (\n",
    "    (df_grouped['Plazo_Simulado_mean'] * df_grouped['Monto_Simulado_mean'] * df_grouped['tasa_optima'] *\n",
    "    ((1 + df_grouped['tasa_optima']) ** df_grouped['Plazo_Simulado_mean'])) /\n",
    "    (((1 + df_grouped['tasa_optima']) ** df_grouped['Plazo_Simulado_mean']) - 1)\n",
    ") - df_grouped['Monto_Simulado_mean']\n",
    "# -------------------------------\n",
    "# Preparación de datos para optimización\n",
    "# -------------------------------\n",
    "# Convertir 'tratamientos' a un arreglo de numpy para mejorar la indexación\n",
    "# Desarrollar y preparar 'tratamientos' para indexación adecuada\n",
    "profits = np.array([\n",
    "    [\n",
    "        row['n_clientes'] * (row['RC'] * (1 - row['Probabilidad_No_Pago']) * row['probabilidad_aceptacion_optima'] * row['tratamientos'][0][t][0]) - \n",
    "        (row['tratamientos'][0][t][1] * costosms)\n",
    "        for t in range(8)\n",
    "    ]\n",
    "    for _, row in df_grouped.iterrows()\n",
    "])\n",
    "# Inicializar el modelo de optimización\n",
    "model = Model(\"Maximizar_Ganancias\")\n",
    "model.ModelSense = GRB.MAXIMIZE\n",
    "# Crear variables de decisión y definir el objetivo\n",
    "n_clients, n_treatments = profits.shape\n",
    "variables = {}\n",
    "for i in range(n_clients):\n",
    "    variables[i] = {}\n",
    "    for t in range(n_treatments):\n",
    "        if profits[i, t] > 0:\n",
    "            variables[i][t] = model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{t}\")\n",
    "model.setObjective(\n",
    "    quicksum(variables[i][t] * profits[i, t] for i in variables for t in variables[i])\n",
    ")\n",
    "# Restricción: Cada cliente recibe exactamente un tratamiento\n",
    "for i in variables:\n",
    "    model.addConstr(quicksum(variables[i].values()) == 1, name=f\"OneTreatmentPerClient_{i}\")\n",
    "# Restricción de capacidad para ejecutivos\n",
    "model.addConstr(\n",
    "    quicksum(variables[i][t] * df_grouped.loc[i, 'n_clientes'] for i in variables for t in variables[i] if t in [5, 6, 7]) <= capacidad_ejecutivos,\n",
    "    name=\"CapacityConstraint\"\n",
    ")\n",
    "# Consistencia de cluster: los clientes dentro del mismo cluster deben recibir el mismo tratamiento\n",
    "clusters = df_grouped.groupby(\"categoria_clusterizacion_numerica\").indices\n",
    "for cluster_id, indices_cluster in clusters.items():\n",
    "    indices_list = list(indices_cluster)\n",
    "    leader_index = indices_list[0]\n",
    "    for t in variables[leader_index]:\n",
    "        leader_var = variables[leader_index][t]\n",
    "        for i in indices_list[1:]:\n",
    "            if t in variables[i]:\n",
    "                model.addConstr(variables[i][t] == leader_var, name=f\"ClusterConsistency_{cluster_id}_{t}\")\n",
    "# Optimizar el modelo\n",
    "model.optimize()\n",
    "# Verificar si la optimización fue exitosa\n",
    "if model.Status == GRB.OPTIMAL:\n",
    "    # -------------------------------\n",
    "    # Extracción y visualización de resultados\n",
    "    # -------------------------------\n",
    "    print(\"Extracting results...\")\n",
    "    # Asignar tratamientos por cluster basado en los resultados de la optimización\n",
    "    resultados_por_cluster = {}\n",
    "    for cluster_id, indices_cluster in clusters.items():\n",
    "        leader_index = list(indices_cluster)[0]\n",
    "        for t in variables[leader_index]:\n",
    "            if variables[leader_index][t].X > 0.5:\n",
    "                resultados_por_cluster[cluster_id] = t + 1\n",
    "                break\n",
    "    # Calcular las ganancias totales\n",
    "    ganancias_totales = model.ObjVal\n",
    "    # Mostrar resultados\n",
    "    print(\"\\nTratamientos asignados por cluster:\")\n",
    "    for cluster_id, tratamiento in resultados_por_cluster.items():\n",
    "        print(f\"Cluster {cluster_id}: Tratamiento {tratamiento}\")\n",
    "    print(f\"\\nGanancias totales: {ganancias_totales:.2f}\")\n",
    "    # Calcular el número de ejecutivos usados y restantes\n",
    "    executives_used = sum(\n",
    "        df_grouped.loc[i, 'n_clientes'] for i in variables for t in variables[i]\n",
    "        if t in [5, 6, 7] and variables[i][t].X > 0.5\n",
    "    )\n",
    "    executives_remaining = capacidad_ejecutivos - executives_used\n",
    "    # Mostrar resumen de uso de ejecutivos\n",
    "    print(f\"\\nExecutives used: {executives_used}\")\n",
    "    print(f\"Executives remaining: {executives_remaining}\")\n",
    "else:\n",
    "    print(\"Optimization did not reach an optimal solution.\")\n",
    "print(\"Optimization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar cuántas veces se asigna cada tratamiento en los resultados por cluster\n",
    "# Se utiliza un diccionario 'Counter' para contar las ocurrencias de cada tratamiento asignado en 'resultados_por_cluster'\n",
    "for cluster_id, tratamiento in resultados_por_cluster.items():\n",
    "    treatment_counts = Counter(resultados_por_cluster.values())\n",
    "\n",
    "# Imprimir el conteo de asignaciones para cada tratamiento\n",
    "# Se recorre 'treatment_counts' para mostrar cuántas veces se asignó cada tratamiento.\n",
    "# 'treatment + 1' se utiliza para mostrar el número de tratamiento en base 1, haciendo el resultado más legible.\n",
    "for treatment, count in treatment_counts.items():\n",
    "    print(f\"Treatment {treatment}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir el diccionario 'resultados_por_cluster' a un DataFrame\n",
    "# El diccionario 'resultados_por_cluster' contiene el ID del cluster y el tratamiento asignado a cada uno.\n",
    "# Se convierte a un DataFrame donde la primera columna es 'cluster' y la segunda 'assigned_treatment'.\n",
    "df_resultados_por_cluster = pd.DataFrame(list(resultados_por_cluster.items()), columns=[\"cluster\", \"assigned_treatment\"])\n",
    "\n",
    "# Mostrar el DataFrame resultante al usuario\n",
    "df_resultados_por_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primer merge con 'df_rut_info'\n",
    "# Realizar una unión ('merge') entre 'df_resultados_por_cluster' y 'df_rut_info' usando 'cluster' en el primer DataFrame\n",
    "# y 'categoria_clusterizacion_numerica' en el segundo como claves de unión.\n",
    "# Esta unión permite agregar información de clientes a cada cluster con su tratamiento asignado.\n",
    "df_assigned = pd.merge(df_resultados_por_cluster, df_rut_info2, left_on='cluster', right_on='categoria_clusterizacion_numerica', how='left')\n",
    "\n",
    "# Segundo merge con 'df_grouped' para agregar la columna 'RC'\n",
    "# Realizar una unión entre 'df_assigned' y 'df_grouped' para incorporar la columna 'RC' (Revenue Calculado)\n",
    "# Usamos 'categoria_clusterizacion_numerica' como clave de unión para añadir la información de revenue calculado a cada cluster.\n",
    "df_assigned = pd.merge(df_assigned, df_grouped[['categoria_clusterizacion_numerica', 'RC']], on='categoria_clusterizacion_numerica', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la probabilidad de simulación para el tratamiento asignado en cada fila de 'df_assigned'\n",
    "# Se usa 'apply' con una función lambda para extraer la probabilidad de simulación correspondiente al tratamiento asignado.\n",
    "# 'row['tratamientos']' es una lista de opciones de tratamiento y 'row['assigned_treatment'] - 1' indica la posición del tratamiento.\n",
    "# Primero, se verifica que 'tratamientos' sea una lista y que el índice calculado esté dentro del rango.\n",
    "# Si estas condiciones se cumplen, se extrae la probabilidad de simulación; en caso contrario, se asigna None.\n",
    "df_assigned['probabilidad_de_simular'] = df_assigned.apply(\n",
    "    lambda row: row['tratamientos'][row['assigned_treatment'] - 1][0] \n",
    "                if isinstance(row['tratamientos'], list) and (0 <= row['assigned_treatment'] - 1 < len(row['tratamientos'])) \n",
    "                else None,\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved in folder: assigned_treatments/assignation_20250617_121747\\assigned_treatments.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Crear una carpeta con un nombre basado en la fecha y hora actual\n",
    "# 'strftime' genera un timestamp en el formato \"YYYYMMDD_HHMMSS\" para asegurar nombres de carpeta únicos.\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "folder_name = f\"assigned_treatments/assignation_{timestamp}\"\n",
    "os.makedirs(folder_name, exist_ok=True)  # Crear la carpeta; 'exist_ok=True' evita errores si la carpeta ya existe.\n",
    "\n",
    "# Definir la ruta del archivo CSV dentro de la nueva carpeta\n",
    "output_path = os.path.join(folder_name, 'assigned_treatments.csv')\n",
    "\n",
    "# Guardar el DataFrame 'df_assigned' con las columnas seleccionadas en un archivo CSV\n",
    "# Se incluyen las columnas clave: 'rut', 'cluster', 'Probabilidad_No_Pago', 'RC', 'assigned_treatment',\n",
    "# 'probabilidad_de_simular', 'tasa_optima' y 'probabilidad_aceptacion_optima'.\n",
    "df_assigned[['rut', 'cluster', 'Probabilidad_No_Pago', 'RC', 'assigned_treatment', 'probabilidad_de_simular', 'tasa_optima', 'probabilidad_aceptacion_optima']].to_csv(output_path, index=False)\n",
    "\n",
    "# Imprimir mensaje de confirmación con la ubicación del archivo CSV guardado\n",
    "print(f\"CSV file saved in folder: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion de la clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración básica del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Nivel de logging (puede ser DEBUG, INFO, WARNING, ERROR, CRITICAL)\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()  # Mostrar logs en la consola\n",
    "        # Puedes agregar FileHandler para guardar logs en un archivo si lo deseas\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)  # Crear un logger\n",
    "\n",
    "# Definir la clase ClusteringEnv\n",
    "class ClusteringEnv(gym.Env):\n",
    "    def __init__(self, data, df_sim_ventas_tratamiento, df_simulaciones_info, cluster_limit: int = 25):\n",
    "        super(ClusteringEnv, self).__init__()\n",
    "        logger.info(\"Inicializando ClusteringEnv con límite de clusters: %d...\", cluster_limit)\n",
    "\n",
    "        # Parámetros de simulación\n",
    "        self.data = data\n",
    "        self.df_sim_ventas_tratamiento = df_sim_ventas_tratamiento\n",
    "        self.df_simulaciones_info = df_simulaciones_info\n",
    "        self.cluster_limit = cluster_limit  # Límite máximo de clusters permitido por episodio\n",
    "\n",
    "        # Inicializar mejores métricas\n",
    "        self.best_reward = float('-inf')\n",
    "        self.best_revenue = float('-inf')\n",
    "\n",
    "        # Variables disponibles (excluyendo 'rut' si existe)\n",
    "        self.variables = [c for c in self.data.columns if c != 'rut']\n",
    "        logger.debug(f\"Variables disponibles: {self.variables}\")\n",
    "\n",
    "        # Identificar variables categóricas y continuas\n",
    "        self.categorical_vars = self.data.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        self.continuous_vars = [c for c in self.data.select_dtypes(include=[np.number]).columns if c != 'rut']\n",
    "        logger.info(f\"Variables categóricas: {self.categorical_vars}\")\n",
    "        logger.info(f\"Variables continuas: {self.continuous_vars}\")\n",
    "\n",
    "        # Parámetros de splits para continuas (min 2, max 3)\n",
    "        self.min_splits = 2\n",
    "        self.max_splits = 3\n",
    "        logger.info(f\"Cortes permitidos por variable continua: entre {self.min_splits} y {self.max_splits}.\")\n",
    "\n",
    "        # Crear acciones posibles\n",
    "        self.included_vars = {}\n",
    "        self.action_list = self.create_action_list()\n",
    "        logger.info(f\"Número de acciones posibles: {len(self.action_list)}\")\n",
    "\n",
    "        # Definir espacios de Gym\n",
    "        self.action_space = spaces.Discrete(len(self.action_list))\n",
    "        state_size = self._compute_state_size()\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(state_size,), dtype=np.float32)\n",
    "        logger.debug(f\"Dimensión del estado: {state_size}\")\n",
    "\n",
    "        # Contadores de episodio\n",
    "        self.current_step = 0\n",
    "        self.max_steps = 20  # tope de acciones por episodio\n",
    "\n",
    "\n",
    "        # Inicializar estado\n",
    "        self.reset()\n",
    "\n",
    "    def _compute_state_size(self):\n",
    "        # Cada variable: 1 indicador de inclusión + max_splits valores si continua\n",
    "        n_vars = len(self.variables)\n",
    "        return n_vars + len(self.continuous_vars) * self.max_splits\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        logger.info(\"Reiniciando el entorno al estado inicial...\")\n",
    "        # Estado previo: no hay variables incluidas, no hay splits\n",
    "        self.included_vars = {var: 0 for var in self.variables}\n",
    "        self.splits = {var: [] for var in self.continuous_vars}\n",
    "        self.current_step = 0\n",
    "        # Reconstruir el estado\n",
    "        self.state = self.get_state()\n",
    "        # Limpiar histórico de mejores métricas si quisieras\n",
    "        # self.best_reward = float('-inf')\n",
    "        # self.best_revenue = float('-inf')\n",
    "        return self.state, {}\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Construye el vector de estado:\n",
    "        # - Para cada variable: indicador de inclusión (0/1)\n",
    "        # - Para variables continuas: valores normalizados de splits (padded a max_splits)\n",
    "        state = []\n",
    "        # Precalcular mínimos y máximos para normalización\n",
    "        min_max = {var: (self.data[var].min(), self.data[var].max()) for var in self.continuous_vars}\n",
    "\n",
    "        for var in self.variables:\n",
    "            included = self.included_vars[var]\n",
    "            state.append(included)\n",
    "\n",
    "            if var in self.continuous_vars:\n",
    "                min_val, max_val = min_max[var]\n",
    "                if included and self.splits[var] and max_val > min_val:\n",
    "                    splits = sorted(self.splits[var])\n",
    "                    normalized = [(s - min_val) / (max_val - min_val) for s in splits]\n",
    "                    # Rellenar con ceros hasta max_splits\n",
    "                    normalized += [0] * (self.max_splits - len(normalized))\n",
    "                    normalized = normalized[:self.max_splits]\n",
    "                else:\n",
    "                    # No incluido o sin splits => ceros\n",
    "                    normalized = [0] * self.max_splits\n",
    "                state.extend(normalized)\n",
    "\n",
    "        return np.array(state, dtype=np.float32)\n",
    "    \n",
    "    def create_action_list(self):\n",
    "            \"\"\"\n",
    "            Genera dinámicamente la lista de acciones posibles según el estado actual:\n",
    "            - toggle_variable sobre cualquier variable (siempre disponible).\n",
    "            - adjust_splits solo para variables continuas que estén incluidas.\n",
    "            \"\"\"\n",
    "            actions = []\n",
    "            # Acciones toggle siempre disponibles\n",
    "            for v in self.variables:\n",
    "                actions.append(('toggle_variable', v, {}))\n",
    "\n",
    "            # Acciones de ajuste de splits solo para continuas incluidas\n",
    "            for v in self.continuous_vars:\n",
    "                if self.included_vars.get(v, 0) == 1:\n",
    "                    # increase / decrease\n",
    "                    actions.append(('adjust_splits', v, {'operation': 'increase'}))\n",
    "                    actions.append(('adjust_splits', v, {'operation': 'decrease'}))\n",
    "                    # move splits\n",
    "                    for idx in range(self.max_splits):\n",
    "                        actions.append(('adjust_splits', v, {'operation': 'move', 'index': idx, 'amount': +1}))\n",
    "                        actions.append(('adjust_splits', v, {'operation': 'move', 'index': idx, 'amount': -1}))\n",
    "            \n",
    "            logger.debug(f\"Acciones generadas: {actions}\")\n",
    "            return actions\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        \"\"\"\n",
    "        1) Actualiza dinámicamente acciones válidas.\n",
    "        2) Proyecta cuántos clusters resultarán combinatoriamente.\n",
    "        3) Si excede cluster_limit, descarta acción (reward=0).\n",
    "        4) Si es viable, aplica acción, calcula reward y actualiza métricas.\n",
    "        \"\"\"\n",
    "        # 1) Refrescar lista de acciones según el estado actual\n",
    "        self.action_list = self.create_action_list()\n",
    "        self.action_space = spaces.Discrete(len(self.action_list))\n",
    "        logger.info(f\"Step {self.current_step+1}/{self.max_steps}\")\n",
    "\n",
    "        # Obtener y simular acción\n",
    "        action = self.action_list[action_index]\n",
    "        temp_inc = self.included_vars.copy()\n",
    "        temp_spl = {v: self.splits[v].copy() for v in self.splits}\n",
    "        self.apply_action(action)\n",
    "\n",
    "        # 2) Proyección combinatoria de clusters\n",
    "        est_clusters = 1\n",
    "        for v, inc in self.included_vars.items():\n",
    "            if not inc:\n",
    "                continue\n",
    "            if v in self.categorical_vars:\n",
    "                est_clusters *= self.data[v].nunique()\n",
    "            else:\n",
    "                est_clusters *= (len(self.splits[v]) + 1)\n",
    "\n",
    "        # Restaurar estado previo\n",
    "        self.included_vars = temp_inc\n",
    "        self.splits = temp_spl\n",
    "\n",
    "        # 3) Verificar límite\n",
    "        if est_clusters > self.cluster_limit:\n",
    "            logger.warning(f\"Inviable: est_clusters={est_clusters} > {self.cluster_limit}\")\n",
    "            return self.state, 0.0, False, False, {'est_clusters': est_clusters, 'violated_limit': True}\n",
    "\n",
    "        # 4) Acción viable: aplicar y proceder\n",
    "        self.apply_action(action)\n",
    "        df = self.perform_clustering()\n",
    "        total_revenue, num_clusters = self.recalculate_metrics(df)\n",
    "        reward = total_revenue\n",
    "        logger.info(f\"Reward={reward}, clusters={num_clusters}\")\n",
    "\n",
    "        # Actualizar mejor configuración\n",
    "        if reward > self.best_reward:\n",
    "            self.best_reward = reward\n",
    "            self.best_revenue = total_revenue\n",
    "            self.best_variables = [v for v, inc in self.included_vars.items() if inc]\n",
    "            self.best_splits = {v: self.splits[v] for v in self.continuous_vars if self.included_vars[v]}\n",
    "\n",
    "        # Avanzar paso y construir retorno\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        self.state = self.get_state()\n",
    "        info = {'total_revenue': total_revenue, 'num_clusters': num_clusters}\n",
    "        return self.state, reward, done, False, info\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        \"\"\"\n",
    "        Aplica 'toggle_variable' o 'adjust_splits' sobre la configuración actual.\n",
    "        \"\"\"\n",
    "        action_type, var, params = action\n",
    "        logger.info(f\"Aplicando acción '{action_type}' sobre variable '{var}' con parámetros {params}\")\n",
    "\n",
    "        if action_type == 'toggle_variable':\n",
    "            # Incluir o excluir la variable\n",
    "            self.included_vars[var] = 1 - self.included_vars[var]\n",
    "            logger.debug(f\"Variable '{var}' estado incluido={self.included_vars[var]}\")\n",
    "            if not self.included_vars[var]:\n",
    "                # Al excluir, eliminamos cortes si existían\n",
    "                self.splits[var] = []\n",
    "\n",
    "        elif action_type == 'adjust_splits':\n",
    "            # Sólo para continuas incluidas\n",
    "            if var not in self.continuous_vars or not self.included_vars.get(var, 0):\n",
    "                logger.warning(f\"No puede ajustar splits para '{var}': no es continua o no está incluida.\")\n",
    "                return\n",
    "\n",
    "            op = params.get('operation')\n",
    "            if op == 'increase':\n",
    "                if len(self.splits[var]) < self.max_splits:\n",
    "                    self.add_split(var)\n",
    "                else:\n",
    "                    logger.warning(f\"Máximo de splits alcanzado para '{var}' ({self.max_splits}).\")\n",
    "\n",
    "            elif op == 'decrease':\n",
    "                if len(self.splits[var]) > self.min_splits:\n",
    "                    self.remove_split(var)\n",
    "                else:\n",
    "                    logger.warning(f\"Mínimo de splits alcanzado para '{var}' ({self.min_splits}).\")\n",
    "\n",
    "            elif op == 'move':\n",
    "                idx = params.get('index', 0)\n",
    "                amt = params.get('amount', 0)\n",
    "                if 0 <= idx < len(self.splits[var]):\n",
    "                    self.move_split(var, idx, amt)\n",
    "                else:\n",
    "                    logger.warning(f\"Índice {idx} inválido para mover split en '{var}'.\")\n",
    "\n",
    "            else:\n",
    "                logger.error(f\"Operación desconocida '{op}' para adjust_splits.\")\n",
    "        else:\n",
    "            logger.error(f\"Tipo de acción desconocido: {action_type}\")\n",
    "    \n",
    "    def sort_splits(self, var):\n",
    "        \"\"\"Ordena ascendentemente los valores de splits[var].\"\"\"\n",
    "        self.splits[var].sort()\n",
    "        logger.debug(f\"Splits ordenados para '{var}': {self.splits[var]}\")\n",
    "\n",
    "    def remove_split(self, var):\n",
    "        \"\"\"Elimina el último split de la lista si existe.\"\"\"\n",
    "        if not self.splits[var]:\n",
    "            logger.warning(f\"No hay splits para eliminar en '{var}'.\")\n",
    "            return\n",
    "        removed = self.splits[var].pop()\n",
    "        logger.info(f\"Split eliminado para '{var}': valor={removed}\")\n",
    "\n",
    "    def add_split(self, var):\n",
    "        \"\"\"Agrega un nuevo split en el punto medio de la mayor brecha disponible.\"\"\"\n",
    "        if len(self.splits[var]) >= self.max_splits:\n",
    "            logger.warning(f\"Máximo de splits ({self.max_splits}) alcanzado en '{var}'.\")\n",
    "            return\n",
    "        current = sorted(self.splits[var])\n",
    "        min_v, max_v = self.data[var].min(), self.data[var].max()\n",
    "        points = [min_v] + current + [max_v]\n",
    "        # Calcular brechas\n",
    "        gaps = [(points[i], points[i+1]) for i in range(len(points)-1)]\n",
    "        a, b = max(gaps, key=lambda x: x[1] - x[0])\n",
    "        new = (a + b) / 2\n",
    "        self.splits[var].append(new)\n",
    "        self.sort_splits(var)\n",
    "        logger.info(f\"Agregado split para '{var}' en {new}\")\n",
    "\n",
    "    def move_split(self, var, index, amount):\n",
    "        \"\"\"Mueve el split en 'index' por 'amount' pasos de tamaño proporcional al rango.\"\"\"\n",
    "        if index < 0 or index >= len(self.splits[var]):\n",
    "            logger.warning(f\"Índice {index} fuera de rango en '{var}'.\")\n",
    "            return\n",
    "        min_v, max_v = self.data[var].min(), self.data[var].max()\n",
    "        step = (max_v - min_v) / 20  # paso de 5% rango\n",
    "        old = self.splits[var][index]\n",
    "        new = min(max(old + amount * step, min_v), max_v)\n",
    "        self.splits[var][index] = new\n",
    "        self.sort_splits(var)\n",
    "        logger.info(f\"Split movido para '{var}' índice {index}: {old} -> {new}\")\n",
    "\n",
    "    def format_splits(self, var):\n",
    "        \"\"\"Devuelve lista de diccionarios con los valores de splits.\"\"\"\n",
    "        return [{'value': v} for v in self.splits[var]]\n",
    "\n",
    "    def perform_clustering(self):\n",
    "        \"\"\"Agrupa el DataFrame según variables incluidas y splits definidos.\"\"\"\n",
    "        logger.info(\"Realizando clustering...\")\n",
    "        df = self.data.copy()\n",
    "        for var in self.variables:\n",
    "            if not self.included_vars[var]:\n",
    "                continue\n",
    "            if var in self.categorical_vars:\n",
    "                df[var + '_cluster'] = df[var]\n",
    "                logger.debug(f\"Categórica '{var}' agrupada por categorías: {df[var + '_cluster'].unique()}\")\n",
    "            else:\n",
    "                splits = sorted(self.splits[var])\n",
    "                if not splits:\n",
    "                    df[var + '_cluster'] = 0\n",
    "                else:\n",
    "                    bins = [-np.inf] + splits + [np.inf]\n",
    "                    labels = [f\"{var}_bin_{i}\" for i in range(len(bins)-1)]\n",
    "                    df[var + '_cluster'] = pd.cut(df[var], bins=bins, labels=labels)\n",
    "                    logger.debug(f\"Splits para '{var}': {self.splits[var]}\")\n",
    "        cluster_cols = [v + '_cluster' for v in self.variables if self.included_vars[v]]\n",
    "        if cluster_cols:\n",
    "            df['category'] = df[cluster_cols].astype(str).agg(' '.join, axis=1)\n",
    "            df['cluster_code'] = df['category'].astype('category').cat.codes\n",
    "            df['categoria_clusterizacion_numerica'] = df['cluster_code']\n",
    "            num = df['cluster_code'].nunique()\n",
    "            logger.info(f\"Clusters formados: {num}\")\n",
    "        else:\n",
    "            df['cluster_code'] = 0\n",
    "            df['categoria_clusterizacion_numerica'] = df['cluster_code']\n",
    "            logger.debug(\"Sin variables incluidas, todos en cluster 0.\")\n",
    "        self.current_clusters = df[['rut', 'cluster_code']]\n",
    "        return df\n",
    "\n",
    "    def function_estimar_elasticidad(self, df_estimar_elasticidad):\n",
    "        logger.info(\"Estimando elasticidad...\")\n",
    "\n",
    "        # Obtener los números únicos de cada cluster\n",
    "        cluster_numbers = df_estimar_elasticidad['categoria_clusterizacion_numerica'].unique()\n",
    "        logger.debug(f\"Número de clusters para estimar elasticidad: {len(cluster_numbers)}\")\n",
    "\n",
    "        # Definir la función que procesará un solo cluster\n",
    "        def process_cluster(cluster_num):\n",
    "            logger.debug(f\"Procesando cluster {cluster_num}...\")\n",
    "            # Filtrar los datos correspondientes al cluster actual\n",
    "            df_cluster = df_estimar_elasticidad[df_estimar_elasticidad['categoria_clusterizacion_numerica'] == cluster_num].copy()\n",
    "\n",
    "            # Asegurarse de que existen datos para ambos casos: venta == 1 y venta == 0\n",
    "            if df_cluster.empty or df_cluster['venta'].isnull().all():\n",
    "                logger.warning(f\"Cluster {cluster_num} está vacío o todas las ventas son nulas. Se salta.\")\n",
    "                return None  # Retornar None para indicar que no hay resultados para este cluster\n",
    "\n",
    "            # Remover filas donde 'venta' o 'Tasa_Simulado' son nulos o infinitos\n",
    "            df_cluster = df_cluster.replace([np.inf, -np.inf], np.nan)\n",
    "            df_cluster = df_cluster.dropna(subset=['venta', 'Tasa_Simulado', 'Plazo_Simulado', 'Monto_Simulado', 'Probabilidad_No_Pago'])\n",
    "\n",
    "            # Saltar el cluster si no hay suficientes puntos de datos\n",
    "            if df_cluster.shape[0] < 10:\n",
    "                logger.warning(f\"Cluster {cluster_num} tiene menos de 10 registros después de limpiar. Se salta.\")\n",
    "                return None\n",
    "\n",
    "            # Extraer las variables 'venta' (como variable dependiente) y 'Tasa_Simulado' (como predictor)\n",
    "            y = df_cluster['venta']\n",
    "            X = df_cluster[['Tasa_Simulado']]\n",
    "\n",
    "            # Añadir un término constante para el intercepto\n",
    "            X = sm.add_constant(X)\n",
    "\n",
    "            # Remover filas con valores NaN o Inf en X o y\n",
    "            is_finite = np.isfinite(X).all(1) & np.isfinite(y)\n",
    "            X = X[is_finite]\n",
    "            y = y[is_finite]\n",
    "\n",
    "            # Asegurarse de que después de remover NaN/Inf, todavía hay suficientes datos\n",
    "            if len(y) < 10:\n",
    "                logger.warning(f\"Cluster {cluster_num} tiene menos de 10 registros después de filtrar finitos. Se salta.\")\n",
    "                return None\n",
    "\n",
    "            # Ajustar el modelo de regresión logística\n",
    "            logit_model = sm.Logit(y, X)\n",
    "            try:\n",
    "                result = logit_model.fit(disp=0)\n",
    "                logger.debug(f\"Modelo ajustado para cluster {cluster_num}.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"No se pudo ajustar el modelo para el cluster {cluster_num}: {e}\")\n",
    "                return None\n",
    "\n",
    "            # Crear una cuadrícula de valores de 'Tasa_Simulado' para predicciones\n",
    "            tasa_min = df_cluster['Tasa_Simulado'].min()\n",
    "            tasa_max = df_cluster['Tasa_Simulado'].max()\n",
    "            tasas_grid = np.linspace(tasa_min, tasa_max, 105)\n",
    "\n",
    "            # Predecir la probabilidad de aceptación usando el modelo ajustado\n",
    "            X_grid = sm.add_constant(tasas_grid)\n",
    "            acceptance_probability = result.predict(X_grid)\n",
    "\n",
    "            # Asegurar que las probabilidades están en el rango [0, 1]\n",
    "            acceptance_probability = np.clip(acceptance_probability, 0, 1)\n",
    "\n",
    "            # Calcular valores medios necesarios para el cálculo de revenue\n",
    "            n = df_cluster['Plazo_Simulado'].mean()\n",
    "            vp = df_cluster['Monto_Simulado'].mean()\n",
    "            pnp = df_cluster['Probabilidad_No_Pago'].mean()\n",
    "            data = {\n",
    "                'Plazo_Simulado_medio': n, \n",
    "                'Monto_Simulado_medio': vp, \n",
    "                'Probabilidad_No_Pago_media': pnp\n",
    "            }\n",
    "\n",
    "            # Calcular el revenue potencial\n",
    "            i = tasas_grid / 100  # Convertir a decimal\n",
    "            one_plus_i_pow_n = np.power(1 + i, n)\n",
    "            annuity_factor = (i * one_plus_i_pow_n) / (one_plus_i_pow_n - 1)\n",
    "            revenue = (n * vp * annuity_factor) - vp\n",
    "            potential_revenue = revenue * (1 - pnp)\n",
    "\n",
    "            # Calcular el promedio de simulaciones por fecha\n",
    "            df_cluster_simulaciones_1 = df_cluster[df_cluster['simulo'] == 1]\n",
    "            num_dates = df_cluster_simulaciones_1['fecha'].nunique()\n",
    "            total_simulaciones = df_cluster_simulaciones_1['simulo'].sum()\n",
    "            simulaciones_medias = total_simulaciones / num_dates if num_dates else 0\n",
    "\n",
    "            # Saltar el cluster si no hay simulaciones\n",
    "            if simulaciones_medias == 0:\n",
    "                logger.warning(f\"Cluster {cluster_num} no tiene simulaciones medias. Se salta.\")\n",
    "                return None\n",
    "\n",
    "            # Calcular el revenue esperado\n",
    "            expected_revenue = acceptance_probability * potential_revenue * simulaciones_medias\n",
    "\n",
    "            # Encontrar la tasa que maximiza el revenue esperado\n",
    "            idx_max = np.argmax(expected_revenue)\n",
    "            max_price = tasas_grid[idx_max]\n",
    "            max_expected_revenue = expected_revenue[idx_max]\n",
    "\n",
    "            # Probabilidad de aceptación en la tasa óptima\n",
    "            prob_aceptacion_optima = acceptance_probability[idx_max]\n",
    "\n",
    "            # Número esperado de créditos aceptados\n",
    "            num_creditos_aceptados = round(prob_aceptacion_optima * simulaciones_medias)\n",
    "\n",
    "            # Número de clientes únicos en el cluster\n",
    "            num_clients = df_cluster['rut'].nunique()\n",
    "\n",
    "            # Imprimir resultados para cada cluster\n",
    "            logger.info(f'Cluster {cluster_num}:')\n",
    "            logger.info(f'- Precio Máx. Revenue Esperado = {max_price:.2f}%')\n",
    "            logger.info(f'- Revenue Esperado Máximo = {max_expected_revenue:,.2f}')\n",
    "            logger.info(f'- Número de clientes en el cluster = {num_clients}')\n",
    "            logger.info(f'- Número de simulaciones en el cluster = {simulaciones_medias:.2f}')\n",
    "            logger.info(f'- Probabilidad de aceptación en el precio óptimo = {prob_aceptacion_optima:.4f}')\n",
    "            logger.info(f'- Número esperado de créditos aceptados = {num_creditos_aceptados}')\n",
    "            logger.info(f'- Monto medio simulado = {data[\"Monto_Simulado_medio\"]:,.2f}')\n",
    "            logger.info(f'- Plazo medio simulado = {data[\"Plazo_Simulado_medio\"]:,.2f}')\n",
    "            logger.info(f'- Probabilidad de no pago media = {data[\"Probabilidad_No_Pago_media\"]:.4f}\\n')\n",
    "\n",
    "            # Preparar los resultados para este cluster\n",
    "            cluster_result = {\n",
    "                'categoria_clusterizacion_numerica': cluster_num,\n",
    "                'tasa_optima': max_price,\n",
    "                'probabilidad_aceptacion_optima': prob_aceptacion_optima,\n",
    "                'revenue_esperado_maximo': max_expected_revenue,\n",
    "                'numero_clientes': num_clients,\n",
    "                'numero_simulaciones_medias': simulaciones_medias,\n",
    "                'numero_creditos_esperados': num_creditos_aceptados,\n",
    "                'monto_medio_simulado': data[\"Monto_Simulado_medio\"],\n",
    "                'plazo_medio_simulado': data[\"Plazo_Simulado_medio\"],\n",
    "                'probabilidad_no_pago_media': data[\"Probabilidad_No_Pago_media\"]\n",
    "            }\n",
    "\n",
    "            return cluster_result\n",
    "\n",
    "        # Ejecutar el procesamiento de clusters en paralelo\n",
    "        num_cores = -1  # Usar todos los núcleos disponibles\n",
    "        results = Parallel(n_jobs=num_cores)(\n",
    "            delayed(process_cluster)(cluster_num) for cluster_num in cluster_numbers\n",
    "        )\n",
    "\n",
    "        # Filtrar los resultados que no son None\n",
    "        cluster_results = [res for res in results if res is not None]\n",
    "\n",
    "        # Si no hay resultados, retornar valores por defecto\n",
    "        if not cluster_results:\n",
    "            logger.warning(\"No se obtuvieron resultados de elasticidad para ningún cluster.\")\n",
    "            total_revenue = 0\n",
    "            total_clientes = 0\n",
    "            total_simulaciones = 0\n",
    "            total_creditos = 0\n",
    "            df_cluster_results = pd.DataFrame()\n",
    "        else:\n",
    "            # Crear un DataFrame a partir de cluster_results\n",
    "            df_cluster_results = pd.DataFrame(cluster_results)\n",
    "\n",
    "            # Imprimir resultados globales\n",
    "            total_revenue = df_cluster_results['revenue_esperado_maximo'].sum()\n",
    "            total_clientes = df_cluster_results['numero_clientes'].sum()\n",
    "            total_simulaciones = df_cluster_results['numero_simulaciones_medias'].sum()\n",
    "            total_creditos = df_cluster_results['numero_creditos_esperados'].sum()\n",
    "\n",
    "            logger.info(f\"El revenue total esperado es: {total_revenue:,.2f} con un total de {total_clientes} clientes, \"\n",
    "                        f\"{total_simulaciones:,.2f} simulaciones, y {total_creditos} créditos.\")\n",
    "\n",
    "        # Incorporar los resultados por cluster de 'df_cluster_results' a 'df_estimar_elasticidad'\n",
    "        if not df_cluster_results.empty:\n",
    "            df_estimar_elasticidad = df_estimar_elasticidad.merge(\n",
    "                df_cluster_results[['categoria_clusterizacion_numerica', 'tasa_optima', 'probabilidad_aceptacion_optima']],\n",
    "                on='categoria_clusterizacion_numerica', \n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            df_estimar_elasticidad['tasa_optima'] = np.nan\n",
    "            df_estimar_elasticidad['probabilidad_aceptacion_optima'] = np.nan\n",
    "\n",
    "        return {\n",
    "            'df_estimar_elasticidad': df_estimar_elasticidad,\n",
    "            'total_revenue': total_revenue,\n",
    "            'total_clientes': total_clientes,\n",
    "            'total_simulaciones': total_simulaciones,\n",
    "            'total_creditos': total_creditos\n",
    "        }\n",
    "    \n",
    "    def function_estimar_respuesta_a_tratamiento(self, df_estimar_elasticidad, df_simulaciones_info): #df1 es df_estimar_elasticidad y df2 es df_simulaciones_info\n",
    "        # Paso 1: Preparación de datos y mapeo de clusters\n",
    "        # Eliminar duplicados en 'df1' para tener un valor único de 'categoria_clusterizacion_numerica' por cada 'rut'.\n",
    "        df_estimar_elasticidad_unique = df_estimar_elasticidad.drop_duplicates(subset='rut')\n",
    "\n",
    "        # Crear un mapeo de 'rut' a 'categoria_clusterizacion_numerica' para asociar cada cliente a su cluster numérico.\n",
    "        rut_cluster_map = df_estimar_elasticidad_unique.set_index('rut')['categoria_clusterizacion_numerica']\n",
    "\n",
    "        # Mapear la categoría de cluster a cada 'rut' en 'df2' usando el mapeo creado\n",
    "        df_simulaciones_info['categoria_clusterizacion_numerica'] = df_simulaciones_info['rut'].map(rut_cluster_map)\n",
    "\n",
    "        # Eliminar filas donde 'categoria_clusterizacion_numerica' es nulo, es decir, aquellos 'rut' sin mapeo de cluster.\n",
    "        df_simulaciones_info = df_simulaciones_info.dropna(subset=['categoria_clusterizacion_numerica'])\n",
    "\n",
    "        # Conversión de tipos de datos\n",
    "        # Convertir 'categoria_clusterizacion_numerica' a entero para garantizar un tipo de dato consistente.\n",
    "        df_simulaciones_info['categoria_clusterizacion_numerica'] = df_simulaciones_info['categoria_clusterizacion_numerica'].astype(int)\n",
    "\n",
    "        # Convertir 'simulo' a numérico, reemplazando valores nulos por 0 y asegurando que sea un tipo de dato entero.\n",
    "        df_simulaciones_info['simulo'] = pd.to_numeric(df_simulaciones_info['simulo'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "        # Convertir 'Tratamiento' a tipo de categoría para optimizar espacio y realizar operaciones categóricas.\n",
    "        df_simulaciones_info['Tratamiento'] = df_simulaciones_info['Tratamiento'].astype('category')\n",
    "\n",
    "        # Paso 2: Calcular el caso total (entradas por tratamiento sin importar el valor de 'simulo')\n",
    "        # Agrupar por 'categoria_clusterizacion_numerica' y 'Tratamiento' para contar el número total de registros en cada combinación.\n",
    "        total_entries_per_cluster_treatment = df_simulaciones_info.groupby(['categoria_clusterizacion_numerica', 'Tratamiento']).size().reset_index(name='caso_total')\n",
    "\n",
    "        # Paso 3: Calcular el caso favorable (entradas por tratamiento cuando 'simulo' == 1)\n",
    "        # Filtrar filas donde 'simulo' es 1 (clientes que realizaron una simulación)\n",
    "        df_simulations = df_simulaciones_info[df_simulaciones_info['simulo'] == 1]\n",
    "\n",
    "        # Agrupar por 'categoria_clusterizacion_numerica' y 'Tratamiento' para contar el número de registros favorables (simulaciones).\n",
    "        favorable_entries_per_cluster_treatment = df_simulations.groupby(['categoria_clusterizacion_numerica', 'Tratamiento']).size().reset_index(name='caso_favorable')\n",
    "\n",
    "        # Paso 4: Calcular la probabilidad de simulación como caso favorable / caso total\n",
    "        # Realizar un merge entre 'total_entries_per_cluster_treatment' y 'favorable_entries_per_cluster_treatment' en las columnas de cluster y tratamiento.\n",
    "        df_probabilities = total_entries_per_cluster_treatment.merge(\n",
    "            favorable_entries_per_cluster_treatment,\n",
    "            on=['categoria_clusterizacion_numerica', 'Tratamiento'],\n",
    "            how='left'\n",
    "        )\n",
    "\n",
    "        # Llenar valores nulos en 'caso_favorable' con 0, asegurando que solo las columnas numéricas estén afectadas.\n",
    "        df_probabilities['caso_favorable'] = df_probabilities['caso_favorable'].fillna(0).astype(int)\n",
    "\n",
    "        # Asegurar que 'caso_total' sea de tipo entero para evitar inconsistencias en los conteos.\n",
    "        df_probabilities['caso_total'] = df_probabilities['caso_total'].astype(int)\n",
    "\n",
    "        # Calcular la probabilidad de simulación como el cociente entre 'caso_favorable' y 'caso_total'.\n",
    "        df_probabilities['probabilidad_simular'] = df_probabilities['caso_favorable'] / df_probabilities['caso_total']\n",
    "\n",
    "        # Organizar las columnas del DataFrame resultante para facilitar su análisis.\n",
    "        df_probabilities = df_probabilities[[\n",
    "            'categoria_clusterizacion_numerica',\n",
    "            'Tratamiento',\n",
    "            'probabilidad_simular',\n",
    "            'caso_favorable',\n",
    "            'caso_total'\n",
    "        ]]\n",
    "\n",
    "        logger.info(\"Estimación de respuesta a tratamientos completada.\")\n",
    "        return df_probabilities\n",
    "\n",
    "    def function_modelo_asignacion_tratamientos(self, df_cluster_info, df_probabilities_treatment, df_rut_info, costo_sms, capacidad_ejecutivos):\n",
    "        logger.info(\"Iniciando modelo de asignación de tratamientos...\")\n",
    "        try:\n",
    "            # Definir el mapeo de tratamientos\n",
    "            tratamiento_map = {  # Mapeo de los tratamientos específicos a identificadores numéricos\n",
    "                \"Ejecutivo=0, Correos=0\": 1, \"Ejecutivo=0, Correos=1\": 2,\n",
    "                \"Ejecutivo=0, Correos=2\": 3, \"Ejecutivo=0, Correos=3\": 4,\n",
    "                \"Ejecutivo=0, Correos=4\": 5, \"Ejecutivo=1, Correos=0\": 6,\n",
    "                \"Ejecutivo=1, Correos=1\": 7, \"Ejecutivo=1, Correos=2\": 8\n",
    "            }\n",
    "\n",
    "            # Parámetros\n",
    "            costosms = costo_sms  # Costo de cada mensaje SMS\n",
    "            capacidad_ejecutivos = capacidad_ejecutivos  # Capacidad máxima en términos de tiempo de los ejecutivos\n",
    "\n",
    "            # Paso 1: Procesar probabilidades y asignar IDs de tratamiento\n",
    "            df_probabilities = df_probabilities_treatment.copy()\n",
    "            df_probabilities['tratamiento_id'] = df_probabilities['Tratamiento'].map(tratamiento_map)\n",
    "            if df_probabilities['tratamiento_id'].isnull().any():\n",
    "                logger.warning(\"Algunos tratamientos no fueron mapeados correctamente a 'tratamiento_id'. Revisar 'tratamiento_map'.\")\n",
    "\n",
    "            # Paso 2: Crear lista de tratamientos y combinar con rut_info\n",
    "            df_probabilities['tratamientos'] = df_probabilities[['probabilidad_simular', 'tratamiento_id']].values.tolist()\n",
    "            grouped_prob = df_probabilities.groupby('categoria_clusterizacion_numerica')['tratamientos'].apply(list).reset_index()\n",
    "\n",
    "            df_rut_info = df_rut_info.copy()\n",
    "            df_rut_info = df_rut_info.merge(grouped_prob, on='categoria_clusterizacion_numerica', how='left')\n",
    "            if df_rut_info['tratamientos'].isnull().any():\n",
    "                logger.warning(\"Algunos clusters no tienen tratamientos asignados.\")\n",
    "\n",
    "            # Paso 3: Combinar rut_info con cluster_info\n",
    "            df_cluster_info = df_cluster_info.copy()\n",
    "            df_rut_info = df_rut_info.merge(df_cluster_info, on='categoria_clusterizacion_numerica', how='left')\n",
    "            if df_rut_info.isnull().any().any():\n",
    "                logger.warning(\"Algunas combinaciones de clusters no están completas en rut_info.\")\n",
    "\n",
    "            # Paso 3.5: Agrupar información por cluster en 'rut_info'\n",
    "            df_grouped = df_rut_info.groupby('categoria_clusterizacion_numerica').agg({\n",
    "                'Probabilidad_No_Pago': 'mean',  # Promedio de probabilidad de no pago\n",
    "                'tratamientos': lambda x: list(x),  # Lista de opciones de tratamiento únicas en cada cluster\n",
    "                'Monto_Simulado_mean': 'mean',\n",
    "                'Plazo_Simulado_mean': 'mean',\n",
    "                'probabilidad_aceptacion_optima': 'mean',\n",
    "                'tasa_optima': 'mean',\n",
    "                'rut': 'count'  # Conteo del número de clientes ('rut') en cada cluster\n",
    "            }).rename(columns={'rut': 'n_clientes'}).reset_index()\n",
    "            logger.debug(f\"Datos agrupados por cluster: {df_grouped.head()}\")\n",
    "\n",
    "            # Paso 4: Calcular 'RC' (Revenue calculado)\n",
    "            df_grouped['tasa_optima'] /= 100  # Convertir tasa óptima a decimal\n",
    "            df_grouped['RC'] = (\n",
    "                (df_grouped['Plazo_Simulado_mean'] * df_grouped['Monto_Simulado_mean'] * df_grouped['tasa_optima'] *\n",
    "                ((1 + df_grouped['tasa_optima']) ** df_grouped['Plazo_Simulado_mean'])) /\n",
    "                (((1 + df_grouped['tasa_optima']) ** df_grouped['Plazo_Simulado_mean']) - 1)\n",
    "            ) - df_grouped['Monto_Simulado_mean']\n",
    "            logger.debug(f\"RC calculado: {df_grouped['RC'].head()}\")\n",
    "\n",
    "            # -------------------------------\n",
    "            # Preparación de datos para optimización\n",
    "            # -------------------------------\n",
    "\n",
    "            try:\n",
    "                # Convertir 'tratamientos' a un arreglo de numpy para mejorar la indexación\n",
    "                profits = np.array([\n",
    "                    [\n",
    "                        row['n_clientes'] * (row['RC'] * (1 - row['Probabilidad_No_Pago']) * row['probabilidad_aceptacion_optima'] * row['tratamientos'][0][t][0]) - \n",
    "                        (row['tratamientos'][0][t][1] * costosms)\n",
    "                        for t in range(8)\n",
    "                    ]\n",
    "                    for _, row in df_grouped.iterrows()\n",
    "                ])\n",
    "                logger.debug(f\"Matriz de beneficios (profits) preparada con forma: {profits.shape}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error al preparar la matriz de beneficios: {e}\")\n",
    "                del df_grouped, df_rut_info, df_probabilities, df_cluster_info\n",
    "                gc.collect()\n",
    "                return {}, 0 # Retornar valores por defecto en caso de error\n",
    "\n",
    "            model = Model(\"Maximizar_Ganancias\")\n",
    "            model.setParam('OutputFlag', 0)  # Disable all output\n",
    "            model.ModelSense = GRB.MAXIMIZE\n",
    "\n",
    "            # Crear variables de decisión y definir el objetivo\n",
    "            n_clients, n_treatments = profits.shape\n",
    "            variables = {}\n",
    "\n",
    "            for i in range(n_clients):\n",
    "                variables[i] = {}\n",
    "                for t in range(n_treatments):\n",
    "                    if profits[i, t] > 0:\n",
    "                        variables[i][t] = model.addVar(vtype=GRB.BINARY, name=f\"x_{i}_{t}\")\n",
    "            logger.debug(f\"Variables de decisión creadas: {len(variables)} clusters con hasta {n_treatments} tratamientos cada uno.\")\n",
    "\n",
    "            # Definir el objetivo\n",
    "            model.setObjective(\n",
    "                quicksum(variables[i][t] * profits[i, t] for i in variables for t in variables[i])\n",
    "            )\n",
    "\n",
    "            # Restricción: Cada cliente recibe exactamente un tratamiento\n",
    "            for i in variables:\n",
    "                model.addConstr(quicksum(variables[i].values()) == 1, name=f\"OneTreatmentPerClient_{i}\")\n",
    "            model.addConstr(\n",
    "                quicksum(variables[i][t] * df_grouped.loc[i, 'n_clientes'] for i in variables for t in variables[i] if t in [5, 6, 7]) <= capacidad_ejecutivos,\n",
    "                name=\"CapacityConstraint\"\n",
    "            )\n",
    "\n",
    "            clusters = df_grouped.groupby(\"categoria_clusterizacion_numerica\").indices\n",
    "            for cluster_id, indices_cluster in clusters.items():\n",
    "                indices_list = list(indices_cluster)\n",
    "                leader_index = indices_list[0]\n",
    "                for t in variables[leader_index]:\n",
    "                    leader_var = variables[leader_index][t]\n",
    "                    for i in indices_list[1:]:\n",
    "                        if t in variables[i]:\n",
    "                            model.addConstr(variables[i][t] == leader_var, name=f\"ClusterConsistency_{cluster_id}_{t}\")\n",
    "            logger.debug(\"Restricciones de consistencia por cluster agregadas.\")\n",
    "\n",
    "            # Optimizar el modelo\n",
    "            try:\n",
    "                model.optimize()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error durante la optimización con Gurobi: {e}\")\n",
    "                return {}, 0  # Retornar valores por defecto en caso de error\n",
    "\n",
    "            # Verificar si la optimización fue exitosa\n",
    "            if model.Status == GRB.OPTIMAL:\n",
    "\n",
    "                # Asignar tratamientos por cluster basado en los resultados de la optimización\n",
    "                resultados_por_cluster = {}\n",
    "                for cluster_id, indices_cluster in clusters.items():\n",
    "                    leader_index = list(indices_cluster)[0]\n",
    "                    for t in variables[leader_index]:\n",
    "                        if variables[leader_index][t].X > 0.5:\n",
    "                            resultados_por_cluster[cluster_id] = t + 1\n",
    "                            break\n",
    "\n",
    "                # Calcular las ganancias totales\n",
    "                ganancias_totales = model.ObjVal\n",
    "                logger.info(f\"Ganancias totales: {ganancias_totales:.2f}\")\n",
    "\n",
    "                # Calcular el número de ejecutivos usados y restantes\n",
    "                executives_used = sum(\n",
    "                    df_grouped.loc[i, 'n_clientes'] for i in variables for t in variables[i]\n",
    "                    if t in [5, 6, 7] and variables[i][t].X > 0.5\n",
    "                )\n",
    "                executives_remaining = capacidad_ejecutivos - executives_used\n",
    "                self.executives_remaining = executives_remaining\n",
    "                logger.info(f\"Executives used: {executives_used}\")\n",
    "                logger.info(f\"Executives remaining: {executives_remaining}\")\n",
    "            else:\n",
    "                logger.error(\"Optimización no alcanzó una solución óptima.\")\n",
    "                resultados_por_cluster = {}\n",
    "                ganancias_totales = 0\n",
    "            logger.info(\"Optimización completada.\")\n",
    "\n",
    "            return resultados_por_cluster, ganancias_totales\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error en function_modelo_asignacion_tratamientos: {e}\")\n",
    "            return {}, 0  # Retornar valores por defecto en caso de error global\n",
    "\n",
    "    def recalculate_metrics(self, df_clusters):\n",
    "        logger.info(\"Recalculando métricas...\")\n",
    "        df_clusters = df_clusters[['rut', 'categoria_clusterizacion_numerica']]\n",
    "\n",
    "        # Fusionar df_clusters con df_simulaciones_e_informacion_de_clientes_ventas_tratamiento\n",
    "        df_estimar_elasticidad = pd.merge(\n",
    "            df_clusters,\n",
    "            self.df_sim_ventas_tratamiento,\n",
    "            on='rut',\n",
    "            how='left'\n",
    "        )\n",
    "        logger.debug(f\"Datos después de la fusión: {df_estimar_elasticidad.head()}\")\n",
    "\n",
    "        dict_elasticidad = self.function_estimar_elasticidad(df_estimar_elasticidad)\n",
    "        df_elasticidad = dict_elasticidad['df_estimar_elasticidad']\n",
    "        logger.info(f\"total_revenue: {dict_elasticidad['total_revenue']}, total_clientes: {dict_elasticidad['total_clientes']}, total_simulaciones: {dict_elasticidad['total_simulaciones']}, total_creditos: {dict_elasticidad['total_creditos']}\")\n",
    "\n",
    "        df_probabilities = self.function_estimar_respuesta_a_tratamiento(df_elasticidad, self.df_simulaciones_info)\n",
    "        logger.debug(f\"Probabilidades calculadas: {df_probabilities.head()}\")\n",
    "\n",
    "        # Calcular los promedios para 'Monto_Simulado' y 'Plazo_Simulado' por categoría\n",
    "        df_elasticidad[['Monto_Simulado_mean', 'Plazo_Simulado_mean']] = df_elasticidad.groupby('categoria_clusterizacion_numerica')[['Monto_Simulado', 'Plazo_Simulado']].transform('mean')\n",
    "        logger.debug(f\"Promedios calculados: {df_elasticidad[['Monto_Simulado_mean', 'Plazo_Simulado_mean']].head()}\")\n",
    "\n",
    "        # Reducir tamaño de los DataFrames a las columnas esenciales\n",
    "        df_estimar_elasticidad_small = df_elasticidad[['categoria_clusterizacion_numerica', 'rut', 'tasa_optima', 'probabilidad_aceptacion_optima', 'Probabilidad_No_Pago', 'Monto_Simulado_mean', 'Plazo_Simulado_mean']]\n",
    "        df_probabilities_small = df_probabilities[['categoria_clusterizacion_numerica', 'probabilidad_simular', 'Tratamiento']]\n",
    "\n",
    "        # Guardar información por cluster, tratamiento y cliente\n",
    "        df_cluster_info = df_estimar_elasticidad_small[['categoria_clusterizacion_numerica', 'Monto_Simulado_mean', 'Plazo_Simulado_mean', 'probabilidad_aceptacion_optima', 'tasa_optima']].drop_duplicates()\n",
    "        df_probabilities_treatment = df_probabilities_small[['categoria_clusterizacion_numerica', 'probabilidad_simular', 'Tratamiento']].drop_duplicates()\n",
    "        df_rut_info = df_estimar_elasticidad_small[['rut', 'categoria_clusterizacion_numerica', 'Probabilidad_No_Pago']].drop_duplicates()\n",
    "\n",
    "        logger.info(\"Llamando al modelo de asignación de tratamientos...\")\n",
    "        # Llamar a la función con los DataFrames procesados\n",
    "        resultados_por_cluster, ganancias_totales = self.function_modelo_asignacion_tratamientos(df_cluster_info, df_probabilities_treatment, df_rut_info, 100, 205000)\n",
    "\n",
    "        logger.info(\"Recalculación de métricas completada.\")\n",
    "        return ganancias_totales, len(resultados_por_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_clientes_rl = df_informacion_de_clientes[['rut', 'Genero', 'Categoria_Digital', 'Elasticidad_Precios', 'Nacionalidad', 'Propension', 'Probabilidad_No_Pago', 'Edad', 'Renta']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 13:33:45,682 - __main__ - INFO - Creando instancia de ClusteringEnv...\n",
      "2025-06-17 13:33:45,683 - __main__ - INFO - Inicializando ClusteringEnv con límite de clusters: 25...\n",
      "2025-06-17 13:33:45,720 - __main__ - INFO - Variables categóricas: ['Genero', 'Categoria_Digital', 'Elasticidad_Precios', 'Nacionalidad']\n",
      "2025-06-17 13:33:45,721 - __main__ - INFO - Variables continuas: ['Propension', 'Probabilidad_No_Pago', 'Edad', 'Renta']\n",
      "2025-06-17 13:33:45,721 - __main__ - INFO - Cortes permitidos por variable continua: entre 2 y 3.\n",
      "2025-06-17 13:33:45,723 - __main__ - INFO - Número de acciones posibles: 8\n",
      "2025-06-17 13:33:45,726 - __main__ - INFO - Reiniciando el entorno al estado inicial...\n",
      "2025-06-17 13:33:45,749 - __main__ - INFO - Verificando el entorno con check_env...\n",
      "2025-06-17 13:33:45,751 - __main__ - INFO - Reiniciando el entorno al estado inicial...\n",
      "2025-06-17 13:33:45,769 - __main__ - INFO - Reiniciando el entorno al estado inicial...\n",
      "2025-06-17 13:33:45,788 - __main__ - INFO - Step 1/20\n",
      "2025-06-17 13:33:45,789 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Renta' con parámetros {}\n",
      "2025-06-17 13:33:45,790 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Renta' con parámetros {}\n",
      "2025-06-17 13:33:45,792 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:33:53,243 - __main__ - INFO - Clusters formados: 1\n",
      "2025-06-17 13:33:53,250 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:33:56,011 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:34:29,721 - __main__ - INFO - El revenue total esperado es: 86,291,395,146.04 con un total de 542904 clientes, 129,035.42 simulaciones, y 67360 créditos.\n",
      "2025-06-17 13:34:31,033 - __main__ - INFO - total_revenue: 86291395146.04265, total_clientes: 542904, total_simulaciones: 129035.42424242424, total_creditos: 67360\n",
      "2025-06-17 13:34:51,927 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:34:55,719 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:34:55,721 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:34:56,160 - __main__ - INFO - Ganancias totales: 63825980369.72\n",
      "2025-06-17 13:34:56,161 - __main__ - INFO - Executives used: 0\n",
      "2025-06-17 13:34:56,162 - __main__ - INFO - Executives remaining: 205000\n",
      "2025-06-17 13:34:56,163 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:34:56,173 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:34:56,827 - __main__ - INFO - Reward=63825980369.71545, clusters=1\n",
      "2025-06-17 13:34:56,896 - __main__ - INFO - Reiniciando el entorno al estado inicial...\n",
      "2025-06-17 13:34:56,944 - __main__ - INFO - Step 1/20\n",
      "2025-06-17 13:34:56,945 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Categoria_Digital' con parámetros {}\n",
      "2025-06-17 13:34:57,025 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Categoria_Digital' con parámetros {}\n",
      "2025-06-17 13:34:57,026 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:35:05,211 - __main__ - INFO - Clusters formados: 2\n",
      "2025-06-17 13:35:05,219 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:35:07,830 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:35:42,551 - __main__ - INFO - El revenue total esperado es: 86,319,923,795.00 con un total de 542904 clientes, 129,035.42 simulaciones, y 67753 créditos.\n",
      "2025-06-17 13:35:43,870 - __main__ - INFO - total_revenue: 86319923794.99782, total_clientes: 542904, total_simulaciones: 129035.42424242424, total_creditos: 67753\n",
      "2025-06-17 13:35:57,955 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:36:01,889 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:36:01,890 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:36:02,435 - __main__ - INFO - Ganancias totales: 63143021807.73\n",
      "2025-06-17 13:36:02,436 - __main__ - INFO - Executives used: 0\n",
      "2025-06-17 13:36:02,437 - __main__ - INFO - Executives remaining: 205000\n",
      "2025-06-17 13:36:02,438 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:36:02,446 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:36:03,087 - __main__ - INFO - Reward=63143021807.72954, clusters=2\n",
      "2025-06-17 13:36:03,115 - __main__ - INFO - Step 2/20\n",
      "2025-06-17 13:36:03,117 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Elasticidad_Precios' con parámetros {}\n",
      "2025-06-17 13:36:03,196 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Elasticidad_Precios' con parámetros {}\n",
      "2025-06-17 13:36:03,197 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:36:11,289 - __main__ - INFO - Clusters formados: 6\n",
      "2025-06-17 13:36:11,296 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:36:13,724 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:36:40,440 - __main__ - INFO - El revenue total esperado es: 106,373,177,450.20 con un total de 542904 clientes, 129,035.42 simulaciones, y 80436 créditos.\n",
      "2025-06-17 13:36:41,786 - __main__ - INFO - total_revenue: 106373177450.20438, total_clientes: 542904, total_simulaciones: 129035.42424242424, total_creditos: 80436\n",
      "2025-06-17 13:36:55,982 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:36:59,791 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:36:59,792 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:37:00,866 - __main__ - INFO - Ganancias totales: 110681217135.98\n",
      "2025-06-17 13:37:00,868 - __main__ - INFO - Executives used: 178322\n",
      "2025-06-17 13:37:00,869 - __main__ - INFO - Executives remaining: 26678\n",
      "2025-06-17 13:37:00,870 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:37:00,878 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:37:01,508 - __main__ - INFO - Reward=110681217135.9761, clusters=6\n",
      "2025-06-17 13:37:01,554 - __main__ - INFO - Step 3/20\n",
      "2025-06-17 13:37:01,555 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Genero' con parámetros {}\n",
      "2025-06-17 13:37:01,671 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Genero' con parámetros {}\n",
      "2025-06-17 13:37:01,672 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:37:09,757 - __main__ - INFO - Clusters formados: 12\n",
      "2025-06-17 13:37:09,764 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:37:12,232 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:37:47,391 - __main__ - INFO - El revenue total esperado es: 106,282,195,162.13 con un total de 542904 clientes, 129,035.42 simulaciones, y 80435 créditos.\n",
      "2025-06-17 13:37:48,699 - __main__ - INFO - total_revenue: 106282195162.12743, total_clientes: 542904, total_simulaciones: 129035.42424242424, total_creditos: 80435\n",
      "2025-06-17 13:38:02,768 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:38:06,833 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:38:06,835 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:38:07,954 - __main__ - INFO - Ganancias totales: 120163556831.15\n",
      "2025-06-17 13:38:07,955 - __main__ - INFO - Executives used: 204521\n",
      "2025-06-17 13:38:07,957 - __main__ - INFO - Executives remaining: 479\n",
      "2025-06-17 13:38:07,957 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:38:07,966 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:38:08,646 - __main__ - INFO - Reward=120163556831.15356, clusters=12\n",
      "2025-06-17 13:38:08,698 - __main__ - INFO - Step 4/20\n",
      "2025-06-17 13:38:08,699 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Categoria_Digital' con parámetros {}\n",
      "2025-06-17 13:38:08,783 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Categoria_Digital' con parámetros {}\n",
      "2025-06-17 13:38:08,786 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:38:16,534 - __main__ - INFO - Clusters formados: 6\n",
      "2025-06-17 13:38:16,541 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:38:19,024 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:38:43,678 - __main__ - INFO - El revenue total esperado es: 106,276,217,308.75 con un total de 542904 clientes, 129,035.42 simulaciones, y 80436 créditos.\n",
      "2025-06-17 13:38:44,897 - __main__ - INFO - total_revenue: 106276217308.75394, total_clientes: 542904, total_simulaciones: 129035.42424242425, total_creditos: 80436\n",
      "2025-06-17 13:38:58,394 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:39:02,099 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:39:02,100 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:39:03,041 - __main__ - INFO - Ganancias totales: 110757852713.27\n",
      "2025-06-17 13:39:03,043 - __main__ - INFO - Executives used: 203518\n",
      "2025-06-17 13:39:03,044 - __main__ - INFO - Executives remaining: 1482\n",
      "2025-06-17 13:39:03,045 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:39:03,054 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:39:03,697 - __main__ - INFO - Reward=110757852713.27081, clusters=6\n",
      "2025-06-17 13:39:03,739 - __main__ - INFO - Step 5/20\n",
      "2025-06-17 13:39:03,740 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Genero' con parámetros {}\n",
      "2025-06-17 13:39:03,775 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Genero' con parámetros {}\n",
      "2025-06-17 13:39:03,776 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:39:10,868 - __main__ - INFO - Clusters formados: 3\n",
      "2025-06-17 13:39:10,874 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:39:13,186 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:39:38,085 - __main__ - INFO - El revenue total esperado es: 106,373,044,596.68 con un total de 542904 clientes, 129,035.42 simulaciones, y 80436 créditos.\n",
      "2025-06-17 13:39:39,290 - __main__ - INFO - total_revenue: 106373044596.67749, total_clientes: 542904, total_simulaciones: 129035.42424242425, total_creditos: 80436\n",
      "2025-06-17 13:39:51,917 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:39:55,651 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:39:55,653 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:39:56,309 - __main__ - INFO - Ganancias totales: 106743822204.90\n",
      "2025-06-17 13:39:56,310 - __main__ - INFO - Executives used: 186659\n",
      "2025-06-17 13:39:56,311 - __main__ - INFO - Executives remaining: 18341\n",
      "2025-06-17 13:39:56,312 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:39:56,321 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:39:56,955 - __main__ - INFO - Reward=106743822204.90375, clusters=3\n",
      "2025-06-17 13:39:56,979 - __main__ - INFO - Step 6/20\n",
      "2025-06-17 13:39:56,981 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Nacionalidad' con parámetros {}\n",
      "2025-06-17 13:39:57,058 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Nacionalidad' con parámetros {}\n",
      "2025-06-17 13:39:57,059 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:40:04,655 - __main__ - INFO - Clusters formados: 6\n",
      "2025-06-17 13:40:04,662 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:40:07,070 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:40:30,123 - __main__ - INFO - El revenue total esperado es: 106,373,303,814.02 con un total de 542904 clientes, 129,035.42 simulaciones, y 80435 créditos.\n",
      "2025-06-17 13:40:31,338 - __main__ - INFO - total_revenue: 106373303814.0168, total_clientes: 542904, total_simulaciones: 129035.42424242425, total_creditos: 80435\n",
      "2025-06-17 13:40:44,892 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:40:48,627 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:40:48,628 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:40:49,513 - __main__ - INFO - Ganancias totales: 108378530968.30\n",
      "2025-06-17 13:40:49,515 - __main__ - INFO - Executives used: 197351\n",
      "2025-06-17 13:40:49,516 - __main__ - INFO - Executives remaining: 7649\n",
      "2025-06-17 13:40:49,518 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:40:49,525 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:40:50,165 - __main__ - INFO - Reward=108378530968.29828, clusters=6\n",
      "2025-06-17 13:40:50,207 - __main__ - INFO - Step 7/20\n",
      "2025-06-17 13:40:50,208 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Propension' con parámetros {}\n",
      "2025-06-17 13:40:50,282 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Propension' con parámetros {}\n",
      "2025-06-17 13:40:50,284 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:40:58,179 - __main__ - INFO - Clusters formados: 6\n",
      "2025-06-17 13:40:58,185 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:41:00,460 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:41:23,848 - __main__ - INFO - El revenue total esperado es: 106,373,303,814.02 con un total de 542904 clientes, 129,035.42 simulaciones, y 80435 créditos.\n",
      "2025-06-17 13:41:25,075 - __main__ - INFO - total_revenue: 106373303814.0168, total_clientes: 542904, total_simulaciones: 129035.42424242425, total_creditos: 80435\n",
      "2025-06-17 13:41:38,109 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:41:41,785 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:41:41,786 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:41:42,666 - __main__ - INFO - Ganancias totales: 108378530968.30\n",
      "2025-06-17 13:41:42,667 - __main__ - INFO - Executives used: 197351\n",
      "2025-06-17 13:41:42,668 - __main__ - INFO - Executives remaining: 7649\n",
      "2025-06-17 13:41:42,670 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:41:42,680 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:41:43,343 - __main__ - INFO - Reward=108378530968.29828, clusters=6\n",
      "2025-06-17 13:41:43,383 - __main__ - INFO - Step 8/20\n",
      "2025-06-17 13:41:43,384 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Categoria_Digital' con parámetros {}\n",
      "2025-06-17 13:41:43,485 - __main__ - INFO - Aplicando acción 'toggle_variable' sobre variable 'Categoria_Digital' con parámetros {}\n",
      "2025-06-17 13:41:43,486 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:41:51,282 - __main__ - INFO - Clusters formados: 12\n",
      "2025-06-17 13:41:51,288 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:41:53,650 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:42:26,831 - __main__ - INFO - El revenue total esperado es: 106,373,348,953.22 con un total de 542904 clientes, 129,035.42 simulaciones, y 80435 créditos.\n",
      "2025-06-17 13:42:28,233 - __main__ - INFO - total_revenue: 106373348953.22461, total_clientes: 542904, total_simulaciones: 129035.42424242423, total_creditos: 80435\n",
      "2025-06-17 13:42:42,324 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:42:46,244 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:42:46,245 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:42:47,093 - __main__ - INFO - Ganancias totales: 120714509325.53\n",
      "2025-06-17 13:42:47,094 - __main__ - INFO - Executives used: 204841\n",
      "2025-06-17 13:42:47,096 - __main__ - INFO - Executives remaining: 159\n",
      "2025-06-17 13:42:47,097 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:42:47,105 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:42:47,750 - __main__ - INFO - Reward=120714509325.5276, clusters=12\n",
      "2025-06-17 13:42:47,794 - __main__ - INFO - Step 9/20\n",
      "2025-06-17 13:42:47,796 - __main__ - INFO - Aplicando acción 'adjust_splits' sobre variable 'Propension' con parámetros {'operation': 'move', 'index': 1, 'amount': -1}\n",
      "2025-06-17 13:42:47,797 - __main__ - WARNING - Índice 1 inválido para mover split en 'Propension'.\n",
      "2025-06-17 13:42:47,912 - __main__ - INFO - Aplicando acción 'adjust_splits' sobre variable 'Propension' con parámetros {'operation': 'move', 'index': 1, 'amount': -1}\n",
      "2025-06-17 13:42:47,914 - __main__ - WARNING - Índice 1 inválido para mover split en 'Propension'.\n",
      "2025-06-17 13:42:47,915 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:42:55,859 - __main__ - INFO - Clusters formados: 12\n",
      "2025-06-17 13:42:55,865 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:42:58,151 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:43:48,162 - __main__ - INFO - El revenue total esperado es: 106,373,348,953.22 con un total de 542904 clientes, 129,035.42 simulaciones, y 80435 créditos.\n",
      "2025-06-17 13:43:49,442 - __main__ - INFO - total_revenue: 106373348953.22461, total_clientes: 542904, total_simulaciones: 129035.42424242423, total_creditos: 80435\n",
      "2025-06-17 13:44:02,919 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:44:06,789 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:44:06,791 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:44:07,678 - __main__ - INFO - Ganancias totales: 120714509325.53\n",
      "2025-06-17 13:44:07,679 - __main__ - INFO - Executives used: 204841\n",
      "2025-06-17 13:44:07,680 - __main__ - INFO - Executives remaining: 159\n",
      "2025-06-17 13:44:07,682 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:44:07,691 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:44:08,324 - __main__ - INFO - Reward=120714509325.5276, clusters=12\n",
      "2025-06-17 13:44:08,372 - __main__ - INFO - Step 10/20\n",
      "2025-06-17 13:44:08,373 - __main__ - INFO - Aplicando acción 'adjust_splits' sobre variable 'Propension' con parámetros {'operation': 'increase'}\n",
      "2025-06-17 13:44:08,380 - __main__ - INFO - Agregado split para 'Propension' en 0.5000001914137039\n",
      "2025-06-17 13:44:08,493 - __main__ - INFO - Aplicando acción 'adjust_splits' sobre variable 'Propension' con parámetros {'operation': 'increase'}\n",
      "2025-06-17 13:44:08,499 - __main__ - INFO - Agregado split para 'Propension' en 0.5000001914137039\n",
      "2025-06-17 13:44:08,501 - __main__ - INFO - Realizando clustering...\n",
      "2025-06-17 13:44:16,461 - __main__ - INFO - Clusters formados: 24\n",
      "2025-06-17 13:44:16,469 - __main__ - INFO - Recalculando métricas...\n",
      "2025-06-17 13:44:18,999 - __main__ - INFO - Estimando elasticidad...\n",
      "2025-06-17 13:45:21,229 - __main__ - INFO - El revenue total esperado es: 106,372,977,989.86 con un total de 542904 clientes, 129,035.42 simulaciones, y 80435 créditos.\n",
      "2025-06-17 13:45:22,741 - __main__ - INFO - total_revenue: 106372977989.85538, total_clientes: 542904, total_simulaciones: 129035.42424242424, total_creditos: 80435\n",
      "2025-06-17 13:45:37,295 - __main__ - INFO - Estimación de respuesta a tratamientos completada.\n",
      "2025-06-17 13:45:41,153 - __main__ - INFO - Llamando al modelo de asignación de tratamientos...\n",
      "2025-06-17 13:45:41,154 - __main__ - INFO - Iniciando modelo de asignación de tratamientos...\n",
      "2025-06-17 13:45:42,126 - __main__ - INFO - Ganancias totales: 120744817208.21\n",
      "2025-06-17 13:45:42,128 - __main__ - INFO - Executives used: 204841\n",
      "2025-06-17 13:45:42,130 - __main__ - INFO - Executives remaining: 159\n",
      "2025-06-17 13:45:42,131 - __main__ - INFO - Optimización completada.\n",
      "2025-06-17 13:45:42,140 - __main__ - INFO - Recalculación de métricas completada.\n",
      "2025-06-17 13:45:42,835 - __main__ - INFO - Reward=120744817208.21182, clusters=24\n",
      "2025-06-17 13:45:42,895 - __main__ - INFO - Inicializando y entrenando el agente DQN...\n",
      "2025-06-17 13:45:43,096 - __main__ - INFO - Inicializando ClusteringEnv con límite de clusters: 25...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 13:45:43,136 - __main__ - INFO - Variables categóricas: ['Genero', 'Categoria_Digital', 'Elasticidad_Precios', 'Nacionalidad']\n",
      "2025-06-17 13:45:43,137 - __main__ - INFO - Variables continuas: ['Propension', 'Probabilidad_No_Pago', 'Edad', 'Renta']\n",
      "2025-06-17 13:45:43,139 - __main__ - INFO - Cortes permitidos por variable continua: entre 2 y 3.\n",
      "2025-06-17 13:45:43,141 - __main__ - INFO - Número de acciones posibles: 8\n",
      "2025-06-17 13:45:43,143 - __main__ - INFO - Reiniciando el entorno al estado inicial...\n",
      "2025-06-17 13:45:43,172 - __main__ - INFO - Iniciando el entrenamiento del agente DQN...\n",
      "2025-06-17 13:45:43,175 - __main__ - INFO - Reiniciando el entorno al estado inicial...\n",
      "2025-06-17 13:45:43,211 - __main__ - INFO - Step 1/20\n",
      "2025-06-17 13:45:43,213 - __main__ - ERROR - Se produjo un error durante la ejecución: list index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\7coto\\AppData\\Local\\Temp\\ipykernel_12304\\172735736.py\", line 124, in <module>\n",
      "    model.learn(\n",
      "  File \"c:\\Users\\7coto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\", line 267, in learn\n",
      "    return super().learn(\n",
      "  File \"c:\\Users\\7coto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 328, in learn\n",
      "    rollout = self.collect_rollouts(\n",
      "  File \"c:\\Users\\7coto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\", line 560, in collect_rollouts\n",
      "    new_obs, rewards, dones, infos = env.step(actions)\n",
      "  File \"c:\\Users\\7coto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py\", line 206, in step\n",
      "    return self.step_wait()\n",
      "  File \"c:\\Users\\7coto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py\", line 58, in step_wait\n",
      "    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n",
      "  File \"c:\\Users\\7coto\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\stable_baselines3\\common\\monitor.py\", line 94, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "  File \"C:\\Users\\7coto\\AppData\\Local\\Temp\\ipykernel_12304\\3594881659.py\", line 143, in step\n",
      "    action = self.action_list[action_index]\n",
      "IndexError: list index out of range\n",
      "2025-06-17 13:45:43,261 - __main__ - INFO - Guardando el modelo debido a una interrupción como 'dqn_clustering_agent_error_20250617_133345.zip'...\n",
      "2025-06-17 13:45:43,337 - __main__ - INFO - Modelo guardado exitosamente como 'dqn_clustering_agent_error_20250617_133345.zip'.\n"
     ]
    }
   ],
   "source": [
    "# Suprimir advertencias de pandas\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "\n",
    "# Configuración básica del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Función para manejar interrupciones y guardar el modelo\n",
    "def save_model_on_interrupt(model, file_name):\n",
    "    logger.info(f\"Guardando el modelo debido a una interrupción como '{file_name}'...\")\n",
    "    model.save(file_name)\n",
    "    logger.info(f\"Modelo guardado exitosamente como '{file_name}'.\")\n",
    "\n",
    "\n",
    "# Clase personalizada de callback para registrar recompensas\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, log_dir, verbose=0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Verificar si ha terminado un episodio\n",
    "        if self.locals.get(\"dones\") and self.locals[\"dones\"][0]:\n",
    "            reward = self.locals[\"rewards\"][0]\n",
    "            length = self.locals[\"infos\"][0].get(\"episode\", {}).get(\"l\", 0)\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_lengths.append(length)\n",
    "            self.episode_count += 1\n",
    "\n",
    "            # Guardar recompensas en un archivo CSV\n",
    "            data = {\n",
    "                \"episode\": self.episode_count,\n",
    "                \"reward\": reward,\n",
    "                \"length\": length,\n",
    "            }\n",
    "            df = pd.DataFrame([data])\n",
    "            csv_file = os.path.join(self.log_dir, \"episode_rewards.csv\")\n",
    "            if not os.path.isfile(csv_file):\n",
    "                df.to_csv(csv_file, index=False)\n",
    "            else:\n",
    "                df.to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "# Configurar el directorio de logs y modelos\n",
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Obtener el timestamp actual\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "try:\n",
    "    # Crear la instancia del entorno\n",
    "    logger.info(\"Creando instancia de ClusteringEnv...\")\n",
    "    env = ClusteringEnv(\n",
    "        df_info_clientes_rl,\n",
    "        df_simulaciones_e_informacion_de_clientes_ventas_tratamiento,\n",
    "        df_simulaciones_info,\n",
    "    )\n",
    "\n",
    "    # Validar si el espacio de acciones está correctamente configurado\n",
    "    if env.action_space.n == 0:\n",
    "        raise ValueError(\n",
    "            \"El espacio de acción está vacío al inicializar el entorno. Revisa la configuración inicial.\"\n",
    "        )\n",
    "\n",
    "    # Envolver el entorno con Monitor para registrar las recompensas\n",
    "    env = Monitor(env, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "\n",
    "    # Verificar el entorno (opcional pero recomendado)\n",
    "    logger.info(\"Verificando el entorno con check_env...\")\n",
    "    check_env(env, warn=True)\n",
    "\n",
    "    # Inicializar y entrenar el agente DQN\n",
    "    logger.info(\"Inicializando y entrenando el agente DQN...\")\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        gamma=0.9,  # Reducir gamma a 0.9\n",
    "        exploration_fraction=1.0,  # Mantener alta la tasa de exploración durante todo el entrenamiento\n",
    "        exploration_final_eps=0.5,  # No disminuir epsilon por debajo de 0.5\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Configurar EvalCallback para evaluar el rendimiento del agente durante el entrenamiento\n",
    "    eval_env = ClusteringEnv(\n",
    "        df_info_clientes_rl,\n",
    "        df_simulaciones_e_informacion_de_clientes_ventas_tratamiento,\n",
    "        df_simulaciones_info,\n",
    "    )\n",
    "    eval_env = Monitor(eval_env, filename=os.path.join(log_dir, \"eval_monitor.csv\"))\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=20,  # Frecuencia ajustada para evaluaciones\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "\n",
    "    # Configurar CheckpointCallback para guardar el modelo periódicamente\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=50,  # Guarda el modelo cada 50 timesteps\n",
    "        save_path=log_dir,\n",
    "        name_prefix=f\"dqn_model_checkpoint_{timestamp}\",\n",
    "    )\n",
    "\n",
    "    # Crear una instancia de RewardLoggerCallback\n",
    "    reward_logger = RewardLoggerCallback(log_dir=log_dir)\n",
    "\n",
    "    # Iniciar el entrenamiento del modelo\n",
    "    logger.info(\"Iniciando el entrenamiento del agente DQN...\")\n",
    "    model.learn(\n",
    "        total_timesteps=100,  # Ajusta el número de timesteps según tus necesidades\n",
    "        callback=[eval_callback, checkpoint_callback, reward_logger],\n",
    "    )\n",
    "    logger.info(\"Entrenamiento del agente DQN completado.\")\n",
    "\n",
    "    # Guardar el modelo entrenado con timestamp\n",
    "    model_file_name = f\"dqn_clustering_agent_{timestamp}.zip\"\n",
    "    model.save(model_file_name)\n",
    "    logger.info(f\"Modelo DQN guardado como '{model_file_name}'.\")\n",
    "\n",
    "    # Cargar las recompensas registradas y graficarlas\n",
    "    logger.info(\"Cargando recompensas y generando gráfica de rendimiento...\")\n",
    "    results_df = pd.read_csv(os.path.join(log_dir, \"monitor.csv\"), skiprows=1)\n",
    "    episode_rewards = results_df[\"r\"].tolist()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"Episodio\")\n",
    "    plt.ylabel(\"Recompensa\")\n",
    "    plt.title(\"Recompensa por Episodio durante el Entrenamiento\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(log_dir, f\"rewards_plot_{timestamp}.png\"))\n",
    "    plt.close()\n",
    "    logger.info(f\"Gráfica de recompensas guardada como 'rewards_plot_{timestamp}.png'.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"Entrenamiento interrumpido por el usuario.\")\n",
    "    # Guardar el modelo con timestamp indicando interrupción\n",
    "    model_file_name = f\"dqn_clustering_agent_interrupt_{timestamp}.zip\"\n",
    "    save_model_on_interrupt(model, file_name=model_file_name)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Se produjo un error durante la ejecución: {str(e)}\", exc_info=True)\n",
    "    # Guardar el modelo con timestamp indicando error\n",
    "    model_file_name = f\"dqn_clustering_agent_error_{timestamp}.zip\"\n",
    "    save_model_on_interrupt(model, file_name=model_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las recompensas registradas y graficarlas\n",
    "results_df = pd.read_csv(os.path.join(log_dir, \"monitor.csv\"), skiprows=1)\n",
    "episode_rewards = results_df[\"r\"].tolist()\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel(\"Episodio\")\n",
    "plt.ylabel(\"Recompensa\")\n",
    "plt.title(\"Recompensa por Episodio durante el Entrenamiento\")\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(log_dir, f\"rewards_plot_{timestamp}.png\"))\n",
    "plt.close()\n",
    "logger.info(f\"Gráfica de recompensas guardada como 'rewards_plot_{timestamp}.png'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Suprimir advertencias de pandas\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
    "\n",
    "# Configuración básica del logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Función para manejar interrupciones y guardar el modelo\n",
    "def save_model_on_interrupt(model, file_name):\n",
    "    logger.info(f\"Guardando el modelo debido a una interrupción como '{file_name}'...\")\n",
    "    model.save(file_name)\n",
    "    logger.info(f\"Modelo guardado exitosamente como '{file_name}'.\")\n",
    "\n",
    "# Clase personalizada de callback para registrar recompensas\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    def __init__(self, log_dir, verbose=0):\n",
    "        super(RewardLoggerCallback, self).__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.episode_count = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Verificar si ha terminado un episodio\n",
    "        if self.locals.get(\"dones\") and self.locals[\"dones\"][0]:\n",
    "            reward = self.locals[\"rewards\"][0]\n",
    "            length = self.locals[\"infos\"][0].get(\"episode\", {}).get(\"l\", 0)\n",
    "            self.episode_rewards.append(reward)\n",
    "            self.episode_lengths.append(length)\n",
    "            self.episode_count += 1\n",
    "\n",
    "            # Guardar recompensas en un archivo CSV\n",
    "            data = {\n",
    "                \"episode\": self.episode_count,\n",
    "                \"reward\": reward,\n",
    "                \"length\": length,\n",
    "            }\n",
    "            df = pd.DataFrame([data])\n",
    "            csv_file = os.path.join(self.log_dir, \"episode_rewards.csv\")\n",
    "            if not os.path.isfile(csv_file):\n",
    "                df.to_csv(csv_file, index=False)\n",
    "            else:\n",
    "                df.to_csv(csv_file, mode=\"a\", header=False, index=False)\n",
    "\n",
    "        return True\n",
    "\n",
    "# Establecer un múltiplo de max_steps para total_timesteps\n",
    "max_steps = 50  # Pasos por episodio\n",
    "episodes_per_block = 5  # Episodios entre evaluaciones\n",
    "eval_freq = max_steps * episodes_per_block  # Evaluar después de 5 episodios\n",
    "save_freq = eval_freq  # Guardar el modelo con la misma frecuencia\n",
    "total_timesteps = eval_freq * 10  # 50 bloques de 5 episodios cada uno\n",
    "\n",
    "# Configurar el directorio de logs y modelos\n",
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Obtener el timestamp actual\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Ruta al modelo DQN existente\n",
    "existing_model_path = \"./dqn_clustering_agent_interrupt_20241202_005345.zip\"\n",
    "try:\n",
    "    # Crear la instancia del entorno\n",
    "    logger.info(\"Creando instancia de ClusteringEnv...\")\n",
    "    env = ClusteringEnv(\n",
    "        df_info_clientes_rl,\n",
    "        df_simulaciones_e_informacion_de_clientes_ventas_tratamiento,\n",
    "        df_simulaciones_info,\n",
    "    )\n",
    "\n",
    "    # Envolver el entorno con Monitor\n",
    "    env = Monitor(env, filename=os.path.join(log_dir, \"monitor.csv\"))\n",
    "\n",
    "    # Verificar el entorno (opcional pero recomendado)\n",
    "    logger.info(\"Verificando el entorno con check_env...\")\n",
    "    check_env(env, warn=True)\n",
    "\n",
    "    # Cargar el modelo DQN existente\n",
    "    logger.info(f\"Cargando el modelo DQN desde '{existing_model_path}'...\")\n",
    "    model = DQN.load(existing_model_path, env=env,\n",
    "                    gamma=0.95,  # Focus on long-term rewards\n",
    "                    exploration_fraction=0.1,  # Quick transition to exploitation\n",
    "                    exploration_final_eps=0.01)  # Minimal randomness in decisions\n",
    "\n",
    "\n",
    "    # Configurar EvalCallback para evaluar el rendimiento del agente durante el entrenamiento\n",
    "    eval_env = ClusteringEnv(\n",
    "        df_info_clientes_rl,\n",
    "        df_simulaciones_e_informacion_de_clientes_ventas_tratamiento,\n",
    "        df_simulaciones_info,\n",
    "    )\n",
    "    eval_env.max_steps = 5\n",
    "    eval_env = Monitor(eval_env, filename=os.path.join(log_dir, \"eval_monitor.csv\"))\n",
    "\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir,\n",
    "        log_path=log_dir,\n",
    "        eval_freq=eval_freq,  # Frecuencia ajustada\n",
    "        deterministic=True,\n",
    "        render=False,\n",
    "    )\n",
    "\n",
    "    # Configurar CheckpointCallback para guardar el modelo periódicamente\n",
    "    checkpoint_callback = CheckpointCallback(\n",
    "        save_freq=save_freq,  # Frecuencia ajustada\n",
    "        save_path=log_dir,\n",
    "        name_prefix=f\"dqn_model_checkpoint_{timestamp}\",\n",
    "    )\n",
    "\n",
    "\n",
    "    # Crear una instancia de RewardLoggerCallback\n",
    "    reward_logger = RewardLoggerCallback(log_dir=log_dir)\n",
    "\n",
    "    # Continuar el entrenamiento del modelo\n",
    "    logger.info(\"Continuando el entrenamiento del modelo DQN...\")\n",
    "    model.learn(\n",
    "        total_timesteps=total_timesteps,  # Total ajustado\n",
    "        callback=[eval_callback, checkpoint_callback, reward_logger],\n",
    "    )\n",
    "    logger.info(\"Entrenamiento del modelo DQN completado.\")\n",
    "\n",
    "    # Guardar el modelo actualizado con timestamp\n",
    "    updated_model_file_name = f\"dqn_clustering_agent_updated_{timestamp}.zip\"\n",
    "    model.save(updated_model_file_name)\n",
    "    logger.info(f\"Modelo DQN actualizado guardado como '{updated_model_file_name}'.\")\n",
    "\n",
    "    # Cargar las recompensas registradas y graficarlas\n",
    "    logger.info(\"Cargando recompensas y generando gráfica de rendimiento...\")\n",
    "    results_df = pd.read_csv(os.path.join(log_dir, \"monitor.csv\"), skiprows=1)\n",
    "    episode_rewards = results_df[\"r\"].tolist()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(episode_rewards)\n",
    "    plt.xlabel(\"Episodio\")\n",
    "    plt.ylabel(\"Recompensa\")\n",
    "    plt.title(\"Recompensa por Episodio durante el Entrenamiento\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(log_dir, f\"rewards_plot_{timestamp}.png\"))\n",
    "    plt.close()\n",
    "    logger.info(f\"Gráfica de recompensas guardada como 'rewards_plot_{timestamp}.png'.\")\n",
    "\n",
    "    # Evaluate the trained model deterministically\n",
    "    logger.info(\"Evaluating the trained model in deterministic mode...\")\n",
    "    deterministic_rewards = []\n",
    "    for _ in range(100):  # Run 100 episodes\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "        deterministic_rewards.append(episode_reward)\n",
    "\n",
    "    average_reward = np.mean(deterministic_rewards)\n",
    "    logger.info(f\"Average reward over 100 deterministic episodes: {average_reward}\")\n",
    "\n",
    "    # Cargar recompensas y generar gráfica de rendimiento\n",
    "    logger.info(\"Cargando recompensas y generando gráfica de rendimiento...\")\n",
    "    results_df = pd.read_csv(os.path.join(log_dir, \"monitor.csv\"), skiprows=1)\n",
    "    episode_rewards = results_df[\"r\"].tolist()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"Entrenamiento interrumpido por el usuario.\")\n",
    "    # Guardar el modelo con timestamp indicando interrupción\n",
    "    interrupted_model_file_name = f\"dqn_clustering_agent_interrupt_{timestamp}.zip\"\n",
    "    save_model_on_interrupt(model, file_name=interrupted_model_file_name)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Se produjo un error durante la ejecución: {str(e)}\", exc_info=True)\n",
    "    # Guardar el modelo con timestamp indicando error\n",
    "    error_model_file_name = f\"dqn_clustering_agent_error_{timestamp}.zip\"\n",
    "    save_model_on_interrupt(model, file_name=error_model_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. SIMULACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos\n",
    "df_clientes = pd.read_csv(\"Informacion_Clientes.csv\")\n",
    "df_cluster = pd.read_csv(\"cluster_data_20241202_201001/cluster_info.csv\")\n",
    "df_treatment = pd.read_csv(\"assigned_treatments/assignation_20241202_202153/assigned_treatments.csv\")\n",
    "\n",
    "print(df_cluster.columns)\n",
    "print(df_treatment.columns)\n",
    "\n",
    "df_cluster.dropna(inplace=True)\n",
    "df_treatment.dropna(inplace=True)\n",
    "\n",
    "\n",
    "# Renombrar categoria_clusterizacion_numerica a cluster\n",
    "df_cluster = df_cluster.rename(columns={'categoria_clusterizacion_numerica': 'cluster'})\n",
    "\n",
    "# Seleccionar solo las columnas adicionales de df_cluster\n",
    "cluster_additional_cols = [col for col in df_cluster.columns if col not in df_treatment.columns and col != 'cluster']\n",
    "\n",
    "\n",
    "# Realizar el merge utilizando solo las columnas adicionales\n",
    "df_combined = df_treatment.merge(\n",
    "    df_cluster[['cluster'] + cluster_additional_cols],  # cluster y columnas adicionales\n",
    "    on='cluster', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(df_combined.columns)\n",
    "\n",
    "# Dividir tasa_optima por 100 directamente en df_combined\n",
    "df_combined['tasa_optima'] = df_combined['tasa_optima'] / 100\n",
    "\n",
    "# Calcular Monto_Simulado en df_clientes\n",
    "df_clientes['Monto_Simulado'] = (\n",
    "    -866900 \n",
    "    + 0.8845 * df_clientes['Renta'] \n",
    "    + 0.7231 * df_clientes['Oferta_Consumo'] \n",
    "    - 0.105 * df_clientes['Deuda_CMF']\n",
    ")\n",
    "\n",
    "# Añadir Monto_Simulado a df_combined\n",
    "df_combined = df_combined.merge(\n",
    "    df_clientes[['rut', 'Monto_Simulado']], \n",
    "    on='rut', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(df_combined.columns)\n",
    "\n",
    "# Reemplazar valores negativos en Monto_Simulado con 0\n",
    "df_combined['Monto_Simulado'] = df_combined['Monto_Simulado'].clip(lower=0)\n",
    "\n",
    "df_combined['Plazo_Esperado'] = (df_combined['Plazo_Simulado_min'] + df_combined['Plazo_Simulado_max'] + df_combined['Plazo_Simulado_mode'])/3\n",
    "\n",
    "# Calcular RC\n",
    "df_combined['RC'] = (\n",
    "    (df_combined['Plazo_Esperado'] * df_combined['Monto_Simulado'] * df_combined['tasa_optima'] *\n",
    "     ((1 + df_combined['tasa_optima']) ** df_combined['Plazo_Esperado'])) /\n",
    "    (((1 + df_combined['tasa_optima']) ** df_combined['Plazo_Esperado']) - 1)\n",
    "    - df_combined['Monto_Simulado']\n",
    ")\n",
    "\n",
    "# Guardar resultado final\n",
    "df_combined.to_csv(\"info_final.csv\", index=False)\n",
    "\n",
    "####################################################################################################\n",
    "# Carga el archivo CSV en un DataFrame\n",
    "df = pd.read_csv('info_final.csv')\n",
    "\n",
    "####################################################################################################\n",
    "# Definimos tambien el costo de los correos\n",
    "costosms = 100\n",
    "\n",
    "# Mapeo del número de correos por tratamiento\n",
    "correos_por_tratamiento = {\n",
    "    1: 0,\n",
    "    2: 1,\n",
    "    3: 2,\n",
    "    4: 3,\n",
    "    5: 4,\n",
    "    6: 0,\n",
    "    7: 1,\n",
    "    8: 2\n",
    "}\n",
    "\n",
    "# Agregar una columna con el número de correos de acuerdo al tratamiento\n",
    "df['num_correos'] = df['assigned_treatment'].map(correos_por_tratamiento)\n",
    "\n",
    "####################################################################################################\n",
    "# PENDIENTE: EVALUAR SI TRABAJAR SIMPLEMENTE CON RC o para cada cluster definir como tratar los \n",
    "# montos y plazos simulados\n",
    "\n",
    "####################################################################################################\n",
    "# SIMULACIÓN y KPIS:\n",
    "# 1.Calcular el valor esperado de ganancias\n",
    "# 2.Calcular el numero promedio de correos enviados\n",
    "# 3.Simular las ventas segun las variables aleatorias (sacar creditos cursados y valor promedio ganancias)\n",
    "# 4.Sacar desviación estandar \n",
    "\n",
    "#####################################################################################################\n",
    "# 1. Valor esperado ganancias (cambiar al tener el estudio de montos y plazos)\n",
    "# Calcular la ganancia esperada para cada fila considerando los costos de correos\n",
    "df['ganancia_esperada_fila'] = (\n",
    "    df['RC'] * \n",
    "    df['probabilidad_de_simular'] * \n",
    "    df['probabilidad_aceptacion_optima'] * \n",
    "    (1 - df['Probabilidad_No_Pago']) -\n",
    "    (df['num_correos'] * costosms)\n",
    ")\n",
    "\n",
    "# Calcular la ganancia_esperada total\n",
    "ganancia_esperada_total = df['ganancia_esperada_fila'].sum()\n",
    "\n",
    "print(\"Ganancia esperada total:\", ganancia_esperada_total)\n",
    "####################################################################################################\n",
    "# 2. Numero de correos enviados:\n",
    "\n",
    "# Calcular el número promedio de correos enviados\n",
    "promedio_correos = df['num_correos'].mean()\n",
    "\n",
    "# Calcular el total de correos enviados\n",
    "total_correos_enviados = df['num_correos'].sum()\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Total de correos enviados:\", total_correos_enviados)\n",
    "print(\"Número promedio de correos enviados:\", promedio_correos)\n",
    "\n",
    "####################################################################################################\n",
    "# 3 y 4. Simulación\n",
    "import numpy as np\n",
    "# Configuración de la simulación\n",
    "num_simulaciones = 100\n",
    "ganancias_simuladas = []\n",
    "tasas_creditos_aceptados = []\n",
    "\n",
    "# Realizar la simulación\n",
    "for _ in range(num_simulaciones):\n",
    "    # Generar plazos simulados con distribución triangular\n",
    "    plazos_simulados = np.random.triangular(\n",
    "        df['Plazo_Simulado_min'],\n",
    "        df['Plazo_Simulado_mode'],\n",
    "        df['Plazo_Simulado_max']\n",
    "    )\n",
    "\n",
    "    # Calcular RC dinámico basado en los plazos simulados\n",
    "    rc_simulado = (\n",
    "        (plazos_simulados * df['Monto_Simulado'] * df['tasa_optima'] *\n",
    "         ((1 + df['tasa_optima']) ** plazos_simulados)) /\n",
    "        (((1 + df['tasa_optima']) ** plazos_simulados) - 1) -\n",
    "        df['Monto_Simulado']\n",
    "    )\n",
    "\n",
    "    # Simular variables aleatorias (binomiales)\n",
    "    sim_probabilidad_de_simular = np.random.binomial(1, df['probabilidad_de_simular'])\n",
    "    sim_probabilidad_aceptacion_optima = np.random.binomial(1, df['probabilidad_aceptacion_optima'])\n",
    "    sim_Probabilidad_No_Pago = np.random.binomial(1, df['Probabilidad_No_Pago'])\n",
    "\n",
    "    # Calcular el total de créditos simulados y aceptados\n",
    "    total_creditos_simulados = sim_probabilidad_de_simular.sum()\n",
    "    total_creditos_aceptados = (sim_probabilidad_de_simular * sim_probabilidad_aceptacion_optima).sum()\n",
    "    \n",
    "    # Calcular la tasa de créditos aceptados\n",
    "    tasa_creditos_aceptados = (\n",
    "        total_creditos_aceptados / total_creditos_simulados\n",
    "        if total_creditos_simulados > 0 else 0\n",
    "    )\n",
    "\n",
    "    # Calcular la ganancia esperada para cada fila en esta simulación\n",
    "    ganancia_simulada_fila = (\n",
    "        rc_simulado *\n",
    "        sim_probabilidad_de_simular *\n",
    "        sim_probabilidad_aceptacion_optima *\n",
    "        (1 - sim_Probabilidad_No_Pago) -\n",
    "        (df['num_correos'] * costosms)\n",
    "    )\n",
    "    \n",
    "    # Calcular la ganancia total de esta simulación y almacenarla\n",
    "    ganancia_total_simulada = ganancia_simulada_fila.sum()\n",
    "    ganancias_simuladas.append(ganancia_total_simulada)\n",
    "    tasas_creditos_aceptados.append(tasa_creditos_aceptados)\n",
    "\n",
    "# Calcular el promedio y la desviación estándar de las ganancias simuladas\n",
    "promedio_ganancias = np.mean(ganancias_simuladas)\n",
    "desviacion_ganancias = np.std(ganancias_simuladas)\n",
    "\n",
    "# Calcular el promedio de la tasa de créditos aceptados\n",
    "promedio_tasa_creditos_aceptados = np.mean(tasas_creditos_aceptados)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Promedio de ganancias simuladas:\", promedio_ganancias)\n",
    "print(\"Desviación estándar de ganancias simuladas:\", desviacion_ganancias)\n",
    "print(\"Tasa promedio de créditos aceptados:\", promedio_tasa_creditos_aceptados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
